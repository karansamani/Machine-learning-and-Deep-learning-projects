

    import pandas as pd
    import numpy as np
    import seaborn as sns
    import matplotlib.pyplot as plt
    from keras.utils.np_utils import to_categorical
    from sklearn.model_selection import KFold, cross_val_score, train_test_split
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.preprocessing import image_dataset_from_directory
    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dense, Dropout, BatchNormalization
    from tensorflow.keras.losses import SparseCategoricalCrossentropy
    from tensorflow.keras.regularizers import l2
    from gc import callbacks
    import sys
    import os
    import cv2 #open cv
    import glob
    from skimage import color

    x_benign="/Users/camron/Documents/IE7615/benign"
    x_malignant="/Users/camron/Documents/IE7615/malignant"

    x_benign_images = [cv2.imread(file) for file in glob.glob(x_benign+'/*.jpg')] #data will be read as a list using imread
    x_malignant_images= [cv2.imread(file) for file in glob.glob(x_malignant+'/*.jpg')]

    print("Number of begnign images =" + str(len(x_benign_images)))
    print("Number of malignant images =" + str(len(x_malignant_images)))

    Number of begnign images =899
    Number of malignant images =1487

    x_benign_images=np.array(x_benign_images) #convert to array of images 
    x_malignant_images=np.array(x_malignant_images)

    print(x_benign_images.shape)
    print(x_malignant_images.shape)

    (899, 224, 224, 3)
    (1487, 224, 224, 3)

    # Create labels
    y_benign = np.zeros(x_benign_images.shape[0]) #target value labels
    y_malignant = np.ones(x_malignant_images.shape[0])

    y_benign

    array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])

    # Merge data 
    X = np.concatenate((x_benign_images, x_malignant_images), axis = 0)
    y = np.concatenate((y_benign, y_malignant), axis = 0) 

    type(y)

    numpy.ndarray

    X.shape

    (2386, 224, 224, 3)

    y.shape

    (2386,)

    y=y.astype(np.int64) #change dtype from float to int
    y

    array([0, 0, 0, ..., 1, 1, 1])

    # one hot encoding

    # y = to_categorical(y, num_classes= 2)
    # y.dtype

    # Shuffle data
    s = np.arange(X.shape[0]) #(start, stop, spacing=1(default))
    np.random.shuffle(s) #shuffles
    X = X[s]
    y= y[s] #here x and y both has same s (index)
    print(s)
    print(y[s])

    [ 631  833 1717 ...    7  178 1058]
    [1 0 0 ... 1 0 1]

    for i in range(1, 15):
      plt.imshow(X[i], interpolation='nearest')
      plt.show()

[]

[]

[]

[]

[]

[]

[]

[]

[]

[]

[]

[]

[]

[]

    X.shape

    (2386, 224, 224, 3)

    fig=plt.figure(figsize=(18, 12))
    columns = 5
    rows = 3

    for i in range(1, columns*rows +1): #1 to 16 range
        ax = fig.add_subplot(rows, columns, i)
        if y[i] == 0:
            ax.title.set_text('Benign')
        else:
            ax.title.set_text('Malignant')
        plt.imshow(X[i], interpolation='nearest')
    plt.show()

[]

    #train test split
    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)

    print("x_train_shape" +str(X_train.shape))
    print("x_test_shape" +str(X_test.shape))
    print("y_train_shape" +str(Y_train.shape))
    print("y_test_shape" +str(Y_test.shape))

    x_train_shape(1908, 224, 224, 3)
    x_test_shape(478, 224, 224, 3)
    y_train_shape(1908,)
    y_test_shape(478,)

    y_unique=np.unique(Y_train, return_counts=True)
    y_unique

    (array([0, 1]), array([ 734, 1174]))

    y_unique[1][0]

    734

    y_unique=np.unique(Y_test, return_counts=True)
    y_unique

    (array([0, 1]), array([165, 313]))

    plt.title("Distribution of target classes")
    sns.countplot(Y_train)

    /Users/camron/opt/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
      warnings.warn(

    <AxesSubplot:title={'center':'Distribution of target classes'}, ylabel='count'>

[]

Data augmentation

    # #data augmentation
    # y_1=np.where(Y_train==1)#malignant images
    # y_1



    # train_ds=np.take(X_train, list(y_1), axis=0) #it applies filter indices, takes only images from those index value=1

    # train_ds

    # train_ds.shape

    # data_augmentation = tf.keras.Sequential(
    #   [
    #     layers.experimental.preprocessing.RandomFlip("horizontal", input_shape=(224, 224,3)),
    #     layers.experimental.preprocessing.RandomRotation(0.1),
    #     layers.experimental.preprocessing.RandomZoom(0.1),
    #   ]
    # )

    # diff=abs(y_unique[1][0]-y_unique[1][1]) # the diff between counts of class 0 and class 1 in Y_train



    # lst=[]
    # for images in train_ds:
    #         augmented_images = data_augmentation(images) #input train_ds images in data augmentation
    #         lst.append(augmented_images)


    # lst=np.array(lst, dtype="uint8") #turning images into array like a vector form

    # lst.shape #new malignant images created as to the value equal in y_train count for malignant images

    # augmented_images_final=np.array(lst[0][0:diff]) #go inside the first dimension and take out the diff (count) number of images, we are not doing random choice.

    # augmented_images_final.shape #final shape 

    # #generating augmented malignant images

    # for i in range(diff): 
    #   plt.imshow(augmented_images_final[i], interpolation='nearest')
    #   plt.title("Malignant")
    #   plt.show()

    # X_train

    # X_train = np.concatenate((X_train, augmented_images_final), axis = 0) #concatenate the actual train data with the new augmented data

    # y_augmented = np.ones(augmented_images_final.shape[0])

    # Y_train = np.concatenate((Y_train, y_augmented), axis = 0)

    # X_train.shape

    # Y_train.shape

    #shuffle for randomness before modeling
    # s = np.arange(X_train.shape[0]) #(start, stop, spacing=1(default))
    # np.random.shuffle(s) #shuffles
    # X_train = X_train[s]
    # Y_train= Y_train[s] #here x and y both has same s (index)
    # print(s)
    # print(Y_train[s])

    y_unique=np.unique(Y_train, return_counts=True) #the class here balanced
    y_unique

    (array([0, 1]), array([ 734, 1174]))

    Y_train=Y_train.astype(np.int64) #
    Y_train

    array([1, 0, 0, ..., 0, 1, 1])

Tensorspec concept of combining image and its label into a tuple

    dataset = tf.data.Dataset.range(2) # (x_train, y_train)
    def g(x):
      return tf.constant(X_train), tf.constant(Y_train) #change to image and labels tuple with constants, unchangeable values
    result_train = dataset.map(g) #dataset creates empty tuple in which we explicitly assign image and the lable
    result_train.element_spec #spec of every element in the component

    2022-04-26 08:42:14.427636: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
    To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.

    (TensorSpec(shape=(1908, 224, 224, 3), dtype=tf.uint8, name=None),
     TensorSpec(shape=(1908,), dtype=tf.int64, name=None))

    dataset = tf.data.Dataset.range(2) # (x_test, y_test)
    def g(x):
      return tf.constant(X_test), tf.constant(Y_test) #change to image and labels tuple with constants, unchangeable values
    result_test = dataset.map(g) #dataset creates empty tuple in which we explicitly assign image and the lable
    result_test.element_spec #spec of every elent in the component

    (TensorSpec(shape=(478, 224, 224, 3), dtype=tf.uint8, name=None),
     TensorSpec(shape=(478,), dtype=tf.int64, name=None))

    ds_train=result_train
    ds_test=result_test

    print(ds_train) #ds_train is map dataset and it will not have shape 

    <MapDataset element_spec=(TensorSpec(shape=(1908, 224, 224, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(1908,), dtype=tf.int64, name=None))>

    print(ds_test)

    <MapDataset element_spec=(TensorSpec(shape=(478, 224, 224, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(478,), dtype=tf.int64, name=None))>

    def normalize_img(image, label):
      """Normalizes images: `uint8` -> `float32`."""
      return tf.cast(image, tf.float32) / 255., label

    #for train data
    ds_train = ds_train.map(
        normalize_img, num_parallel_calls=tf.data.AUTOTUNE) #autotune with the runtime
    ds_train = ds_train.cache() #cache transformation can cache a dataset, either in memory or on local storage. This will save some operations (like file opening and data reading) from being executed during each epoch.
    ds_train = ds_train.shuffle(X_train.shape[0]) #reshuffle on train , For true randomness, set the shuffle buffer to the full dataset size.
    ds_train = ds_train.batch(128) #Batch elements of the dataset after shuffling to get unique batches at each epoch
    ds_train = ds_train.prefetch(tf.data.AUTOTUNE) #This allows later elements to be prepared while the current element is being processed.

    #same for test data
    ds_test = ds_test.map(
        normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
    ds_test = ds_test.batch(128)
    ds_test = ds_test.cache()
    ds_test = ds_test.prefetch(tf.data.AUTOTUNE)

    print(ds_train)

    <PrefetchDataset element_spec=(TensorSpec(shape=(None, 1908, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1908), dtype=tf.int64, name=None))>

    print(ds_test)

    <PrefetchDataset element_spec=(TensorSpec(shape=(None, 478, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 478), dtype=tf.int64, name=None))>

    X_train.shape

    (1908, 224, 224, 3)

Modeling

Baseline Model

    # # Augmenting the image data

    augment = tf.keras.Sequential(
        [
    #     layers.RandomFlip("horizontal"),
    #     #layers.RandomRotation(.1),
    layers.RandomContrast(factor = .2),
    #     layers.GaussianNoise(.01)
        ]
    )
    lst = []
    for images in X_train:
        aug_x = augment(images)
        lst.append(aug_x)
        
    X_train = np.array(lst, dtype="uint8")   

    # # Normalizing the image data

    X_train=X_train/255.0

    model = Sequential()

    #add model layers
    model.add(Conv2D(filters=16, kernel_size=(3,3), strides=(1,1), padding='same', activation="relu", input_shape=(224,224,3)))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding='same', activation="relu"))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same', activation="relu"))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Dropout(0.15))

    model.add(Flatten()) #we have two layers after the flatening
    model.add(Dense(128, activation="relu"))
    model.add(Dense(2, activation="softmax"))

    model.compile(optimizer=tf.keras.optimizers.Adam(0.00001),
                  loss=SparseCategoricalCrossentropy(from_logits=True),
                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])

    from tensorflow.keras.utils import to_categorical, plot_model
    plot_model(model, to_file='cnn-mnist.png', show_shapes=True) #shows the summary of the model

    You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.

    from keras.callbacks import EarlyStopping #early stopping
    es=EarlyStopping(
        monitor='val_sparse_categorical_accuracy',
        min_delta=0,
        patience=100,
        verbose=1,
        mode='auto',
        baseline=None,
        restore_best_weights=True
    )

    history=model.fit(
        X_train,Y_train,
        epochs=25, #can change the epoch
        validation_split=0.15, verbose=1,callbacks=[es])

    Epoch 1/25

    /Users/camron/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: "`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?"
      return dispatch_target(*args, **kwargs)

    51/51 [==============================] - 22s 424ms/step - loss: 0.6466 - sparse_categorical_accuracy: 0.6336 - val_loss: 0.6229 - val_sparse_categorical_accuracy: 0.7143
    Epoch 2/25
    51/51 [==============================] - 22s 425ms/step - loss: 0.5940 - sparse_categorical_accuracy: 0.7286 - val_loss: 0.5861 - val_sparse_categorical_accuracy: 0.7422
    Epoch 3/25
    51/51 [==============================] - 22s 426ms/step - loss: 0.5534 - sparse_categorical_accuracy: 0.7742 - val_loss: 0.5458 - val_sparse_categorical_accuracy: 0.7631
    Epoch 4/25
    51/51 [==============================] - 22s 427ms/step - loss: 0.5132 - sparse_categorical_accuracy: 0.7952 - val_loss: 0.5109 - val_sparse_categorical_accuracy: 0.7840
    Epoch 5/25
    51/51 [==============================] - 22s 427ms/step - loss: 0.4730 - sparse_categorical_accuracy: 0.8162 - val_loss: 0.4845 - val_sparse_categorical_accuracy: 0.8049
    Epoch 6/25
    51/51 [==============================] - 22s 426ms/step - loss: 0.4498 - sparse_categorical_accuracy: 0.8291 - val_loss: 0.4623 - val_sparse_categorical_accuracy: 0.8118
    Epoch 7/25
    51/51 [==============================] - 22s 428ms/step - loss: 0.4285 - sparse_categorical_accuracy: 0.8359 - val_loss: 0.4454 - val_sparse_categorical_accuracy: 0.8153
    Epoch 8/25
    51/51 [==============================] - 22s 428ms/step - loss: 0.4168 - sparse_categorical_accuracy: 0.8365 - val_loss: 0.4315 - val_sparse_categorical_accuracy: 0.8188
    Epoch 9/25
    51/51 [==============================] - 22s 428ms/step - loss: 0.4043 - sparse_categorical_accuracy: 0.8341 - val_loss: 0.4388 - val_sparse_categorical_accuracy: 0.8049
    Epoch 10/25
    51/51 [==============================] - 22s 429ms/step - loss: 0.3993 - sparse_categorical_accuracy: 0.8378 - val_loss: 0.4249 - val_sparse_categorical_accuracy: 0.8049
    Epoch 11/25
    51/51 [==============================] - 22s 431ms/step - loss: 0.3862 - sparse_categorical_accuracy: 0.8415 - val_loss: 0.4166 - val_sparse_categorical_accuracy: 0.8153
    Epoch 12/25
    51/51 [==============================] - 22s 430ms/step - loss: 0.3807 - sparse_categorical_accuracy: 0.8433 - val_loss: 0.4150 - val_sparse_categorical_accuracy: 0.8118
    Epoch 13/25
    51/51 [==============================] - 22s 428ms/step - loss: 0.3749 - sparse_categorical_accuracy: 0.8445 - val_loss: 0.4112 - val_sparse_categorical_accuracy: 0.8084
    Epoch 14/25
    51/51 [==============================] - 22s 429ms/step - loss: 0.3677 - sparse_categorical_accuracy: 0.8464 - val_loss: 0.4218 - val_sparse_categorical_accuracy: 0.8084
    Epoch 15/25
    51/51 [==============================] - 22s 436ms/step - loss: 0.3721 - sparse_categorical_accuracy: 0.8390 - val_loss: 0.3956 - val_sparse_categorical_accuracy: 0.8153
    Epoch 16/25
    51/51 [==============================] - 22s 433ms/step - loss: 0.3604 - sparse_categorical_accuracy: 0.8495 - val_loss: 0.3939 - val_sparse_categorical_accuracy: 0.8188
    Epoch 17/25
    51/51 [==============================] - 22s 432ms/step - loss: 0.3605 - sparse_categorical_accuracy: 0.8526 - val_loss: 0.3920 - val_sparse_categorical_accuracy: 0.8223
    Epoch 18/25
    51/51 [==============================] - 22s 432ms/step - loss: 0.3535 - sparse_categorical_accuracy: 0.8439 - val_loss: 0.3908 - val_sparse_categorical_accuracy: 0.8188
    Epoch 19/25
    51/51 [==============================] - 22s 437ms/step - loss: 0.3522 - sparse_categorical_accuracy: 0.8513 - val_loss: 0.3852 - val_sparse_categorical_accuracy: 0.8258
    Epoch 20/25
    51/51 [==============================] - 24s 462ms/step - loss: 0.3539 - sparse_categorical_accuracy: 0.8439 - val_loss: 0.3915 - val_sparse_categorical_accuracy: 0.8223
    Epoch 21/25
    51/51 [==============================] - 23s 444ms/step - loss: 0.3482 - sparse_categorical_accuracy: 0.8556 - val_loss: 0.4185 - val_sparse_categorical_accuracy: 0.8084
    Epoch 22/25
    51/51 [==============================] - 22s 440ms/step - loss: 0.3459 - sparse_categorical_accuracy: 0.8482 - val_loss: 0.3816 - val_sparse_categorical_accuracy: 0.8188
    Epoch 23/25
    51/51 [==============================] - 22s 429ms/step - loss: 0.3441 - sparse_categorical_accuracy: 0.8532 - val_loss: 0.3824 - val_sparse_categorical_accuracy: 0.8223
    Epoch 24/25
    51/51 [==============================] - 22s 431ms/step - loss: 0.3432 - sparse_categorical_accuracy: 0.8519 - val_loss: 0.3850 - val_sparse_categorical_accuracy: 0.8293
    Epoch 25/25
    51/51 [==============================] - 22s 427ms/step - loss: 0.3381 - sparse_categorical_accuracy: 0.8489 - val_loss: 0.3829 - val_sparse_categorical_accuracy: 0.8328

    plt.plot(history.history['sparse_categorical_accuracy'])
    plt.plot(history.history['val_sparse_categorical_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()

[]

    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()

[]

    # np.unique(Y_train)

    # Testing model on test data to evaluate
    y_pred = model.predict(X_test)
    y_pred #it defaulty arranges class 0 prob at index 0 and for class 1 at index 1

    array([[1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 2.82275643e-19],
           [1.00000000e+00, 9.15309772e-11],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 4.42833259e-35],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [9.99997020e-01, 2.94367896e-06],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [8.76684412e-02, 9.12331581e-01],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 8.35263583e-16],
           [1.00000000e+00, 0.00000000e+00],
           [6.43835730e-24, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 1.96338625e-31],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 1.03960542e-30],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 7.04082077e-38],
           [1.00000000e+00, 1.29413777e-08],
           [1.00000000e+00, 0.00000000e+00],
           [9.99999285e-01, 6.58999738e-07],
           [9.25578962e-32, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [2.30612356e-29, 1.00000000e+00],
           [2.77787236e-25, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 3.82546141e-29],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [6.08784349e-29, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.15522719e-03, 9.98844743e-01],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 1.49825625e-20],
           [2.11449155e-21, 1.00000000e+00],
           [9.99999881e-01, 9.29795974e-08],
           [1.00000000e+00, 0.00000000e+00],
           [9.99998331e-01, 1.61290359e-06],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 8.00813999e-17],
           [1.00000000e+00, 4.68894909e-29],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 5.08169049e-24],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [9.99999881e-01, 1.17417805e-07],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [7.17596278e-31, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [3.94305434e-23, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 3.28518267e-12],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 1.37275824e-15],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [4.83877652e-19, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 4.62880839e-14],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 7.17144349e-17],
           [1.00000000e+00, 0.00000000e+00],
           [9.99991179e-01, 8.81522919e-06],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [7.24553267e-22, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 1.40213402e-34],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.92962750e-35, 1.00000000e+00],
           [1.54835025e-17, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [2.09930349e-19, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [9.85668606e-24, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [2.73355296e-18, 1.00000000e+00],
           [2.52474175e-32, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 2.33639145e-27],
           [1.00000000e+00, 0.00000000e+00],
           [1.45837483e-31, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [9.99995947e-01, 4.03540980e-06],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.08758534e-18, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 7.43590356e-13],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [3.56673697e-32, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 8.67927064e-09],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 4.06911329e-34],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [3.86737374e-04, 9.99613345e-01],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.16984380e-18, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 9.87216716e-21],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 1.50796948e-20],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [4.67492541e-23, 1.00000000e+00],
           [1.00000000e+00, 1.64265561e-24],
           [6.82364592e-13, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 1.41994584e-33],
           [3.71241880e-30, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [5.44835233e-21, 1.00000000e+00],
           [1.16270121e-29, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.45347905e-04, 9.99854684e-01],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [6.47994881e-24, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 1.20306982e-28],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.27249240e-24, 1.00000000e+00],
           [1.00000000e+00, 4.23395168e-27],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.78184225e-12, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 3.07720717e-17],
           [1.00000000e+00, 0.00000000e+00],
           [7.72585225e-35, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 1.09888753e-32],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 1.47141028e-18],
           [1.00000000e+00, 1.72532327e-14],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [3.77430089e-14, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [2.05629494e-29, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 7.80987406e-16],
           [1.00000000e+00, 0.00000000e+00],
           [9.99990344e-01, 9.60729176e-06],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [9.30517495e-01, 6.94825351e-02],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [9.99999762e-01, 2.85340633e-07],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 2.22846676e-18],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [5.75319203e-32, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 1.28999016e-08],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [9.99970675e-01, 2.92670702e-05],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 5.31423668e-26],
           [0.00000000e+00, 1.00000000e+00],
           [9.99549091e-01, 4.50864085e-04],
           [1.00000000e+00, 0.00000000e+00],
           [1.00000000e+00, 0.00000000e+00]], dtype=float32)

    lst=[]

    for i in range(0,len(y_pred)):
         k=np.argmax(y_pred[i]) #it gives index value of the highest probability for each iteration 
         print(k)
         lst.append(k)

    y_pred_label=np.array(lst)

    0
    1
    1
    0
    0
    1
    1
    1
    1
    0
    0
    0
    1
    0
    0
    1
    0
    0
    0
    0
    0
    0
    0
    0
    1
    0
    0
    1
    0
    1
    0
    0
    0
    0
    0
    0
    1
    0
    0
    1
    0
    0
    1
    1
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    1
    1
    0
    1
    0
    0
    0
    1
    1
    1
    1
    0
    0
    0
    0
    0
    0
    1
    1
    1
    0
    0
    1
    0
    0
    0
    0
    1
    1
    0
    0
    0
    0
    0
    1
    0
    0
    0
    1
    0
    0
    0
    1
    0
    0
    0
    0
    0
    0
    0
    1
    1
    0
    0
    0
    0
    0
    0
    0
    0
    1
    0
    0
    0
    0
    0
    0
    1
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    1
    1
    0
    0
    0
    1
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    1
    1
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    1
    0
    0
    0
    0
    0
    0
    1
    0
    0
    0
    0
    0
    0
    0
    1
    0
    0
    0
    0
    0
    0
    1
    0
    1
    1
    1
    1
    1
    0
    1
    0
    1
    1
    0
    0
    1
    1
    1
    0
    0
    0
    1
    1
    0
    0
    0
    1
    1
    1
    0
    0
    0
    0
    1
    0
    0
    0
    1
    1
    0
    1
    0
    1
    0
    0
    1
    0
    0
    0
    0
    0
    1
    1
    0
    0
    1
    0
    0
    0
    0
    0
    0
    1
    0
    1
    1
    0
    1
    0
    1
    0
    1
    0
    0
    1
    0
    0
    1
    0
    0
    0
    0
    0
    0
    1
    1
    1
    0
    0
    0
    1
    1
    1
    0
    0
    0
    0
    0
    0
    0
    0
    1
    0
    0
    0
    1
    0
    1
    0
    1
    1
    0
    0
    0
    0
    0
    1
    0
    0
    1
    0
    1
    1
    1
    0
    0
    0
    0
    0
    1
    0
    1
    0
    0
    1
    0
    0
    1
    0
    1
    0
    0
    0
    0
    0
    1
    0
    0
    0
    0
    0
    1
    0
    0
    1
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    1
    0
    0
    0
    1
    1
    0
    0
    1
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    1
    0
    0
    0
    1
    0
    0
    0
    0
    0
    1
    1
    0
    1
    0
    0
    0
    0
    1
    0
    0
    0
    0
    0
    0
    0
    0
    0
    1
    0
    0
    0
    1
    0
    0
    1
    0
    0
    0
    1
    1
    0
    1
    1
    0
    0
    1
    1
    0
    0
    0
    0
    0
    0
    0
    0
    1
    0
    0
    1
    1
    0
    0
    0
    0
    0
    0
    1
    1
    0
    0
    0
    1
    1
    1
    0
    1
    0
    1
    0
    1
    0
    0
    0
    1
    1
    0
    1
    0
    1
    0
    0
    0
    1
    0
    0
    1
    0
    0
    0

    # type(Y_test)

    from sklearn.metrics import confusion_matrix
    from sklearn.metrics import plot_confusion_matrix

    mat = confusion_matrix(Y_test, y_pred_label) #we dont do this because we dont get the whole number on the confusion matrixis to fet the whole number annotation
    sns.heatmap(mat, square=True, annot=True, cbar=False)
    plt.xlabel('predicted value')
    plt.ylabel('true value');

[]

    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


    print('Accuracy: %.3f' % accuracy_score(y_true=Y_test, y_pred=y_pred_label))
    print('Precision: %.3f' % precision_score(y_true=Y_test, y_pred=y_pred_label))
    print('Recall: %.3f' % recall_score(y_true=Y_test, y_pred=y_pred_label))
    print('F1: %.3f' % f1_score(y_true=Y_test, y_pred=y_pred_label))

    Accuracy: 0.615
    Precision: 0.971
    Recall: 0.425
    F1: 0.591

    from sklearn import metrics

    # Model f1_score: how often is the classifier correct?
    baseline_f1_score=metrics.f1_score(Y_test, y_pred_label)



    print("F1_score:",baseline_f1_score)

    F1_score: 0.5911111111111111

    from sklearn.metrics import roc_curve
    from sklearn.metrics import auc
    fpr_keras, tpr_keras, thresholds_keras = roc_curve(Y_test, y_pred_label)


    auc_keras_baseline = auc(fpr_keras, tpr_keras)
    auc_keras_baseline

    0.7003388517765515

    import matplotlib.pyplot as plt
    plt.title('Baseline_Model-Receiver Operating Characteristic')
    plt.plot(fpr_keras, tpr_keras, color='green',marker='o', label = 'AUC area = %0.2f' % auc_keras_baseline)
    plt.legend(loc = 'lower right')
    plt.plot([0, 1], [0, 1],'r--') #diagonal line
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')

    Text(0.5, 0, 'False Positive Rate')

[]

    # from tensorflow.keras.models import load_model

    # reloaded_model=load_model("/content/drive/My Drive/my_model.h5") #loading the h5 file model

Model-ResNet 50

    from tensorflow.keras.applications.resnet50 import ResNet50
    from tensorflow.keras import Model

    input_shape=(224,224,3)



    head_model = ResNet50(include_top=False,
                     weights='imagenet', #random initialization
                     #input_tensor=None,
                     input_shape=input_shape)
                     #pooling='avg',
                     #classes=2)


    for layer in head_model.layers:
      layer.trainable = False #trainable are the last three layers until flatten (the whole set of fully connected layers)


    x = layers.Flatten()(head_model.output) #google: how to cut off a pre train model resnet and add fully connected layers in tensorflow
    x = layers.Dense(1000, activation='relu')(x)
    predictions = layers.Dense(2, activation = 'softmax')(x)

    model = Model(inputs = head_model.input, outputs = predictions)




    model.compile(optimizer=tf.keras.optimizers.Adam(0.00001),
                  loss=SparseCategoricalCrossentropy(from_logits=True),
                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])

    history=model.fit(
        X_train,Y_train,
        epochs=25, #can change the epoch
        validation_split=0.15, verbose=1,callbacks=[es])

    Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5
    94773248/94765736 [==============================] - 2s 0us/step
    94781440/94765736 [==============================] - 2s 0us/step
    Epoch 1/25

    /Users/camron/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: "`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?"
      return dispatch_target(*args, **kwargs)

    51/51 [==============================] - 124s 2s/step - loss: 0.7338 - sparse_categorical_accuracy: 0.5719 - val_loss: 0.6311 - val_sparse_categorical_accuracy: 0.6760
    Epoch 2/25
    51/51 [==============================] - 116s 2s/step - loss: 0.6594 - sparse_categorical_accuracy: 0.5891 - val_loss: 0.6396 - val_sparse_categorical_accuracy: 0.6341
    Epoch 3/25
    51/51 [==============================] - 117s 2s/step - loss: 0.6076 - sparse_categorical_accuracy: 0.6589 - val_loss: 0.5617 - val_sparse_categorical_accuracy: 0.6760
    Epoch 4/25
    51/51 [==============================] - 118s 2s/step - loss: 0.6058 - sparse_categorical_accuracy: 0.6767 - val_loss: 0.5463 - val_sparse_categorical_accuracy: 0.7526
    Epoch 5/25
    51/51 [==============================] - 114s 2s/step - loss: 0.5683 - sparse_categorical_accuracy: 0.7218 - val_loss: 0.5303 - val_sparse_categorical_accuracy: 0.7317
    Epoch 6/25
    51/51 [==============================] - 114s 2s/step - loss: 0.5560 - sparse_categorical_accuracy: 0.7242 - val_loss: 0.5394 - val_sparse_categorical_accuracy: 0.7631
    Epoch 7/25
    51/51 [==============================] - 116s 2s/step - loss: 0.5549 - sparse_categorical_accuracy: 0.7082 - val_loss: 0.6915 - val_sparse_categorical_accuracy: 0.5226
    Epoch 8/25
    51/51 [==============================] - 116s 2s/step - loss: 0.5366 - sparse_categorical_accuracy: 0.7458 - val_loss: 0.5001 - val_sparse_categorical_accuracy: 0.7979
    Epoch 9/25
    51/51 [==============================] - 115s 2s/step - loss: 0.5440 - sparse_categorical_accuracy: 0.7168 - val_loss: 0.6520 - val_sparse_categorical_accuracy: 0.5749
    Epoch 10/25
    51/51 [==============================] - 116s 2s/step - loss: 0.5650 - sparse_categorical_accuracy: 0.6940 - val_loss: 0.5327 - val_sparse_categorical_accuracy: 0.7038
    Epoch 11/25
    51/51 [==============================] - 116s 2s/step - loss: 0.5342 - sparse_categorical_accuracy: 0.7298 - val_loss: 0.4829 - val_sparse_categorical_accuracy: 0.7944
    Epoch 12/25
    51/51 [==============================] - 117s 2s/step - loss: 0.5041 - sparse_categorical_accuracy: 0.7785 - val_loss: 0.5433 - val_sparse_categorical_accuracy: 0.6829
    Epoch 13/25
    51/51 [==============================] - 120s 2s/step - loss: 0.5246 - sparse_categorical_accuracy: 0.7242 - val_loss: 0.5011 - val_sparse_categorical_accuracy: 0.7352
    Epoch 14/25
    51/51 [==============================] - 119s 2s/step - loss: 0.4842 - sparse_categorical_accuracy: 0.7977 - val_loss: 0.4687 - val_sparse_categorical_accuracy: 0.7979
    Epoch 15/25
    51/51 [==============================] - 121s 2s/step - loss: 0.5137 - sparse_categorical_accuracy: 0.7489 - val_loss: 0.4658 - val_sparse_categorical_accuracy: 0.8014
    Epoch 16/25
    51/51 [==============================] - 120s 2s/step - loss: 0.4765 - sparse_categorical_accuracy: 0.7983 - val_loss: 0.4827 - val_sparse_categorical_accuracy: 0.7700
    Epoch 17/25
    51/51 [==============================] - 120s 2s/step - loss: 0.5008 - sparse_categorical_accuracy: 0.7471 - val_loss: 0.5306 - val_sparse_categorical_accuracy: 0.6934
    Epoch 18/25
    51/51 [==============================] - 122s 2s/step - loss: 0.4740 - sparse_categorical_accuracy: 0.7909 - val_loss: 0.4524 - val_sparse_categorical_accuracy: 0.8153
    Epoch 19/25
    51/51 [==============================] - 114s 2s/step - loss: 0.4737 - sparse_categorical_accuracy: 0.7791 - val_loss: 0.4693 - val_sparse_categorical_accuracy: 0.7840
    Epoch 20/25
    51/51 [==============================] - 118s 2s/step - loss: 0.4884 - sparse_categorical_accuracy: 0.7539 - val_loss: 0.4697 - val_sparse_categorical_accuracy: 0.7596
    Epoch 21/25
    51/51 [==============================] - 119s 2s/step - loss: 0.4666 - sparse_categorical_accuracy: 0.8020 - val_loss: 0.5094 - val_sparse_categorical_accuracy: 0.7143
    Epoch 22/25
    51/51 [==============================] - 123s 2s/step - loss: 0.4728 - sparse_categorical_accuracy: 0.7847 - val_loss: 0.4399 - val_sparse_categorical_accuracy: 0.8084
    Epoch 23/25
    51/51 [==============================] - 123s 2s/step - loss: 0.4536 - sparse_categorical_accuracy: 0.7964 - val_loss: 0.5076 - val_sparse_categorical_accuracy: 0.7143
    Epoch 24/25
    51/51 [==============================] - 120s 2s/step - loss: 0.4493 - sparse_categorical_accuracy: 0.8149 - val_loss: 0.4353 - val_sparse_categorical_accuracy: 0.8049
    Epoch 25/25
    51/51 [==============================] - 119s 2s/step - loss: 0.5073 - sparse_categorical_accuracy: 0.7428 - val_loss: 0.5437 - val_sparse_categorical_accuracy: 0.7038

    plt.plot(history.history['sparse_categorical_accuracy'])
    plt.plot(history.history['val_sparse_categorical_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()

[]

    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()

[]

    y_pred=model.predict(X_test) 
    y_pred

    array([[8.98574851e-03, 9.91014302e-01],
           [3.21432458e-06, 9.99996781e-01],
           [3.04428363e-06, 9.99996901e-01],
           [5.23867842e-04, 9.99476135e-01],
           [5.67383692e-03, 9.94326174e-01],
           [1.90025894e-05, 9.99981046e-01],
           [2.36244440e-01, 7.63755620e-01],
           [3.84158455e-02, 9.61584151e-01],
           [3.43208849e-05, 9.99965668e-01],
           [2.41662329e-03, 9.97583389e-01],
           [8.76785489e-04, 9.99123275e-01],
           [1.65746933e-05, 9.99983430e-01],
           [1.53108337e-03, 9.98468935e-01],
           [2.44814041e-03, 9.97551858e-01],
           [1.51844390e-04, 9.99848127e-01],
           [7.11019254e-07, 9.99999285e-01],
           [1.59847587e-02, 9.84015226e-01],
           [3.96533730e-03, 9.96034682e-01],
           [4.09568841e-08, 1.00000000e+00],
           [1.41131941e-05, 9.99985933e-01],
           [1.75609239e-04, 9.99824464e-01],
           [4.18526324e-05, 9.99958158e-01],
           [8.16338797e-06, 9.99991894e-01],
           [6.93007139e-03, 9.93069947e-01],
           [2.01222792e-04, 9.99798834e-01],
           [1.01605394e-07, 9.99999881e-01],
           [4.45143627e-07, 9.99999523e-01],
           [1.53272020e-04, 9.99846697e-01],
           [9.35293176e-10, 1.00000000e+00],
           [6.45853106e-07, 9.99999404e-01],
           [2.08919300e-06, 9.99997854e-01],
           [2.37459084e-04, 9.99762595e-01],
           [7.83295371e-03, 9.92167056e-01],
           [5.71340434e-02, 9.42865968e-01],
           [7.11335379e-05, 9.99928832e-01],
           [4.93211473e-07, 9.99999523e-01],
           [9.56171080e-02, 9.04382885e-01],
           [3.51931012e-05, 9.99964833e-01],
           [2.25086667e-04, 9.99774873e-01],
           [2.51560181e-04, 9.99748409e-01],
           [5.76540060e-07, 9.99999404e-01],
           [2.33378869e-05, 9.99976635e-01],
           [5.06166741e-03, 9.94938374e-01],
           [7.95136486e-07, 9.99999166e-01],
           [1.84527980e-05, 9.99981523e-01],
           [5.98597296e-07, 9.99999404e-01],
           [2.00645445e-07, 9.99999762e-01],
           [5.37893455e-03, 9.94621038e-01],
           [6.45910278e-02, 9.35408950e-01],
           [9.04010423e-03, 9.90959883e-01],
           [3.19899805e-03, 9.96801019e-01],
           [8.08335415e-07, 9.99999166e-01],
           [8.80806474e-05, 9.99911904e-01],
           [2.57914944e-04, 9.99742091e-01],
           [6.07312795e-05, 9.99939322e-01],
           [6.95203244e-06, 9.99993086e-01],
           [5.93020286e-06, 9.99994040e-01],
           [8.71562133e-06, 9.99991298e-01],
           [1.93325963e-04, 9.99806702e-01],
           [1.78787914e-07, 9.99999881e-01],
           [2.84014666e-03, 9.97159839e-01],
           [2.10508279e-06, 9.99997854e-01],
           [2.48986129e-02, 9.75101352e-01],
           [8.26810574e-05, 9.99917269e-01],
           [2.60717981e-03, 9.97392774e-01],
           [7.92843124e-08, 9.99999881e-01],
           [3.86113256e-01, 6.13886654e-01],
           [3.64624981e-07, 9.99999642e-01],
           [3.43172769e-06, 9.99996543e-01],
           [2.69939285e-03, 9.97300684e-01],
           [5.59483422e-04, 9.99440491e-01],
           [8.09790848e-08, 9.99999881e-01],
           [4.00884630e-04, 9.99599159e-01],
           [5.94343533e-07, 9.99999404e-01],
           [5.12050246e-06, 9.99994874e-01],
           [1.94924220e-01, 8.05075765e-01],
           [1.01311679e-03, 9.98986900e-01],
           [6.60040129e-08, 9.99999881e-01],
           [1.31359688e-04, 9.99868631e-01],
           [1.52125489e-02, 9.84787524e-01],
           [3.80746729e-04, 9.99619246e-01],
           [7.25999940e-04, 9.99273956e-01],
           [1.22001437e-10, 1.00000000e+00],
           [6.99886903e-02, 9.30011272e-01],
           [2.71823723e-04, 9.99728143e-01],
           [8.35135486e-03, 9.91648674e-01],
           [9.63372386e-08, 9.99999881e-01],
           [3.47335845e-05, 9.99965310e-01],
           [1.74387493e-09, 1.00000000e+00],
           [1.17420814e-05, 9.99988198e-01],
           [1.67035716e-04, 9.99832988e-01],
           [8.81995632e-08, 9.99999881e-01],
           [1.16327582e-02, 9.88367260e-01],
           [2.24272907e-02, 9.77572680e-01],
           [1.79016411e-01, 8.20983529e-01],
           [3.69495916e-04, 9.99630451e-01],
           [3.84020086e-06, 9.99996185e-01],
           [1.90658554e-01, 8.09341431e-01],
           [7.45215046e-04, 9.99254763e-01],
           [1.09526212e-03, 9.98904705e-01],
           [1.80721434e-03, 9.98192847e-01],
           [6.20741361e-08, 9.99999881e-01],
           [5.16399741e-04, 9.99483585e-01],
           [9.81175708e-06, 9.99990225e-01],
           [2.58431141e-03, 9.97415662e-01],
           [1.61720498e-03, 9.98382807e-01],
           [7.71916565e-03, 9.92280781e-01],
           [1.57595772e-04, 9.99842405e-01],
           [2.84577909e-05, 9.99971509e-01],
           [1.14930532e-04, 9.99885082e-01],
           [2.21353676e-03, 9.97786403e-01],
           [6.81595120e-04, 9.99318361e-01],
           [2.17777565e-01, 7.82222450e-01],
           [2.32835009e-06, 9.99997616e-01],
           [1.64538065e-07, 9.99999881e-01],
           [1.11200556e-04, 9.99888778e-01],
           [5.26962367e-06, 9.99994755e-01],
           [3.04175094e-02, 9.69582558e-01],
           [9.56116468e-02, 9.04388368e-01],
           [2.88790255e-03, 9.97112036e-01],
           [7.40311880e-05, 9.99925971e-01],
           [1.91864208e-03, 9.98081326e-01],
           [3.61678831e-04, 9.99638319e-01],
           [1.77844902e-06, 9.99998212e-01],
           [7.82186191e-08, 9.99999881e-01],
           [1.41250237e-03, 9.98587489e-01],
           [5.88843599e-03, 9.94111598e-01],
           [6.02169393e-06, 9.99993920e-01],
           [1.92389078e-02, 9.80761051e-01],
           [2.89346031e-06, 9.99997139e-01],
           [5.44424430e-02, 9.45557535e-01],
           [8.65942147e-03, 9.91340578e-01],
           [1.53403217e-02, 9.84659672e-01],
           [2.05150442e-04, 9.99794900e-01],
           [1.65615333e-04, 9.99834299e-01],
           [2.72253619e-05, 9.99972820e-01],
           [6.33124262e-04, 9.99366939e-01],
           [3.59273166e-03, 9.96407211e-01],
           [2.91043310e-04, 9.99708951e-01],
           [3.05654190e-04, 9.99694347e-01],
           [2.77073644e-02, 9.72292602e-01],
           [2.42255755e-05, 9.99975801e-01],
           [7.95151354e-05, 9.99920487e-01],
           [6.79520053e-06, 9.99993205e-01],
           [1.11646317e-01, 8.88353646e-01],
           [2.92090408e-04, 9.99707878e-01],
           [8.26645846e-05, 9.99917388e-01],
           [9.85000770e-06, 9.99990106e-01],
           [2.16421988e-02, 9.78357792e-01],
           [7.64963450e-03, 9.92350399e-01],
           [2.64039863e-05, 9.99973655e-01],
           [1.60219934e-04, 9.99839783e-01],
           [4.03556012e-04, 9.99596417e-01],
           [1.31530140e-03, 9.98684704e-01],
           [3.55182920e-06, 9.99996424e-01],
           [5.09429839e-04, 9.99490499e-01],
           [1.86334873e-05, 9.99981403e-01],
           [1.40279444e-05, 9.99985933e-01],
           [9.19353351e-06, 9.99990821e-01],
           [6.33941283e-07, 9.99999404e-01],
           [1.06095197e-02, 9.89390552e-01],
           [4.64359910e-05, 9.99953508e-01],
           [1.31925563e-05, 9.99986768e-01],
           [2.11062958e-03, 9.97889340e-01],
           [6.69344008e-05, 9.99933004e-01],
           [5.11517428e-05, 9.99948859e-01],
           [4.25090911e-05, 9.99957442e-01],
           [2.33666037e-06, 9.99997616e-01],
           [1.25722854e-05, 9.99987483e-01],
           [2.99560325e-03, 9.97004449e-01],
           [1.76602152e-05, 9.99982357e-01],
           [1.91397376e-05, 9.99980807e-01],
           [3.80702669e-09, 1.00000000e+00],
           [3.72140761e-07, 9.99999642e-01],
           [2.03095796e-03, 9.97968972e-01],
           [1.26858184e-04, 9.99873161e-01],
           [3.67577275e-04, 9.99632359e-01],
           [2.30481359e-03, 9.97695148e-01],
           [2.07566191e-04, 9.99792397e-01],
           [1.04091182e-01, 8.95908892e-01],
           [7.21571196e-05, 9.99927878e-01],
           [9.50807414e-04, 9.99049127e-01],
           [2.16042636e-07, 9.99999762e-01],
           [3.36348785e-05, 9.99966383e-01],
           [1.39099150e-08, 1.00000000e+00],
           [1.65240328e-06, 9.99998331e-01],
           [2.68252479e-05, 9.99973178e-01],
           [8.02177994e-04, 9.99197781e-01],
           [4.15601506e-04, 9.99584377e-01],
           [1.61674325e-04, 9.99838352e-01],
           [1.82520689e-05, 9.99981761e-01],
           [3.36478797e-06, 9.99996662e-01],
           [4.92833408e-07, 9.99999523e-01],
           [1.31915629e-04, 9.99868035e-01],
           [2.32923683e-02, 9.76707637e-01],
           [3.74835763e-05, 9.99962568e-01],
           [2.73494632e-03, 9.97265100e-01],
           [1.16865259e-07, 9.99999881e-01],
           [6.39968584e-06, 9.99993563e-01],
           [7.58200400e-08, 9.99999881e-01],
           [4.31902939e-04, 9.99568045e-01],
           [1.46577136e-07, 9.99999881e-01],
           [3.71190526e-06, 9.99996305e-01],
           [8.63288940e-10, 1.00000000e+00],
           [5.93142817e-04, 9.99406815e-01],
           [2.62311577e-08, 1.00000000e+00],
           [5.76945720e-04, 9.99423027e-01],
           [3.40309963e-02, 9.65968966e-01],
           [2.36463640e-03, 9.97635365e-01],
           [1.24173928e-02, 9.87582624e-01],
           [3.35657301e-11, 1.00000000e+00],
           [1.55206874e-01, 8.44793081e-01],
           [1.10547766e-02, 9.88945186e-01],
           [1.99065489e-05, 9.99980092e-01],
           [6.87120846e-05, 9.99931335e-01],
           [3.20788899e-07, 9.99999642e-01],
           [7.17425646e-05, 9.99928236e-01],
           [9.63736996e-02, 9.03626323e-01],
           [3.74713773e-03, 9.96252775e-01],
           [1.83400587e-07, 9.99999762e-01],
           [4.16003866e-04, 9.99584019e-01],
           [2.83027535e-09, 1.00000000e+00],
           [5.39326167e-04, 9.99460638e-01],
           [8.33238482e-06, 9.99991655e-01],
           [1.18305277e-08, 1.00000000e+00],
           [1.88915408e-04, 9.99811113e-01],
           [5.09091515e-05, 9.99949098e-01],
           [1.88146643e-07, 9.99999762e-01],
           [4.62543039e-06, 9.99995351e-01],
           [3.54895927e-03, 9.96451020e-01],
           [4.99390124e-04, 9.99500632e-01],
           [1.47382077e-03, 9.98526096e-01],
           [3.36043042e-04, 9.99663949e-01],
           [6.97697327e-02, 9.30230260e-01],
           [1.28739816e-03, 9.98712659e-01],
           [3.14374774e-05, 9.99968529e-01],
           [6.85622171e-03, 9.93143797e-01],
           [5.31772166e-05, 9.99946833e-01],
           [2.49436707e-05, 9.99975085e-01],
           [2.54567993e-07, 9.99999762e-01],
           [1.05384048e-02, 9.89461601e-01],
           [2.28803056e-05, 9.99977112e-01],
           [4.73521603e-03, 9.95264769e-01],
           [8.14949235e-05, 9.99918461e-01],
           [2.66100524e-05, 9.99973416e-01],
           [8.70439089e-06, 9.99991298e-01],
           [4.02596706e-05, 9.99959707e-01],
           [9.88629926e-03, 9.90113735e-01],
           [8.53938855e-06, 9.99991417e-01],
           [1.32865330e-06, 9.99998689e-01],
           [8.65296030e-08, 9.99999881e-01],
           [5.03309187e-04, 9.99496698e-01],
           [1.11770896e-05, 9.99988794e-01],
           [4.05855710e-03, 9.95941460e-01],
           [3.21982592e-01, 6.78017378e-01],
           [1.79758026e-05, 9.99981999e-01],
           [7.19851914e-06, 9.99992847e-01],
           [7.41171977e-03, 9.92588341e-01],
           [2.92682435e-05, 9.99970675e-01],
           [5.94944595e-06, 9.99994040e-01],
           [5.02495141e-06, 9.99994993e-01],
           [6.06922712e-03, 9.93930817e-01],
           [1.43701764e-04, 9.99856234e-01],
           [2.97671977e-06, 9.99997020e-01],
           [8.23109440e-06, 9.99991775e-01],
           [1.00907585e-08, 1.00000000e+00],
           [2.18962669e-01, 7.81037331e-01],
           [5.56758821e-01, 4.43241239e-01],
           [1.79822352e-02, 9.82017756e-01],
           [5.84030295e-06, 9.99994159e-01],
           [7.70919607e-04, 9.99229074e-01],
           [3.86025012e-02, 9.61397529e-01],
           [1.59127176e-05, 9.99984145e-01],
           [2.10473740e-06, 9.99997854e-01],
           [3.87499604e-04, 9.99612510e-01],
           [6.77601621e-03, 9.93224025e-01],
           [7.29877320e-06, 9.99992728e-01],
           [1.06888102e-07, 9.99999881e-01],
           [1.56092619e-06, 9.99998450e-01],
           [5.97900653e-04, 9.99402046e-01],
           [4.08440679e-01, 5.91559291e-01],
           [1.95469397e-06, 9.99998093e-01],
           [1.04258868e-06, 9.99998927e-01],
           [1.68289280e-05, 9.99983191e-01],
           [5.21495276e-05, 9.99947906e-01],
           [3.89588095e-09, 1.00000000e+00],
           [8.09411517e-07, 9.99999166e-01],
           [1.20006756e-04, 9.99879956e-01],
           [1.21822224e-04, 9.99878168e-01],
           [5.88267518e-04, 9.99411702e-01],
           [7.92829553e-08, 9.99999881e-01],
           [8.28802968e-07, 9.99999166e-01],
           [7.50724494e-01, 2.49275565e-01],
           [5.58261236e-05, 9.99944210e-01],
           [6.86719868e-05, 9.99931335e-01],
           [2.31888443e-01, 7.68111587e-01],
           [2.15495834e-06, 9.99997854e-01],
           [3.07090668e-04, 9.99692917e-01],
           [1.15894691e-05, 9.99988437e-01],
           [7.62306008e-05, 9.99923706e-01],
           [3.76941716e-05, 9.99962330e-01],
           [6.34304129e-08, 9.99999881e-01],
           [1.11606383e-07, 9.99999881e-01],
           [4.64161771e-04, 9.99535918e-01],
           [1.25603729e-05, 9.99987483e-01],
           [4.64137685e-07, 9.99999523e-01],
           [3.58758825e-06, 9.99996424e-01],
           [3.43472237e-08, 1.00000000e+00],
           [1.46030034e-05, 9.99985337e-01],
           [7.50450864e-02, 9.24954832e-01],
           [5.75935655e-06, 9.99994278e-01],
           [3.64405923e-02, 9.63559330e-01],
           [5.22230715e-02, 9.47776973e-01],
           [4.23833844e-04, 9.99576151e-01],
           [2.77773157e-04, 9.99722183e-01],
           [8.23677146e-06, 9.99991775e-01],
           [3.99265009e-05, 9.99960065e-01],
           [1.19874656e-07, 9.99999881e-01],
           [6.64579915e-04, 9.99335349e-01],
           [2.64861574e-03, 9.97351408e-01],
           [7.14325643e-06, 9.99992847e-01],
           [6.02110390e-07, 9.99999404e-01],
           [6.31911025e-05, 9.99936819e-01],
           [2.92905534e-05, 9.99970675e-01],
           [1.87953740e-01, 8.12046289e-01],
           [2.10615917e-06, 9.99997854e-01],
           [3.75321193e-04, 9.99624610e-01],
           [1.78139731e-02, 9.82186019e-01],
           [2.82516144e-03, 9.97174859e-01],
           [5.51974168e-03, 9.94480252e-01],
           [1.84781551e-02, 9.81521845e-01],
           [6.69229962e-03, 9.93307710e-01],
           [3.32057352e-05, 9.99966741e-01],
           [6.91481400e-05, 9.99930859e-01],
           [4.20356577e-04, 9.99579608e-01],
           [8.29935800e-08, 9.99999881e-01],
           [1.37230280e-04, 9.99862790e-01],
           [1.49613870e-02, 9.85038638e-01],
           [5.65594746e-07, 9.99999404e-01],
           [2.23309456e-04, 9.99776661e-01],
           [4.05489355e-02, 9.59451020e-01],
           [9.08372167e-05, 9.99909163e-01],
           [1.70042682e-02, 9.82995689e-01],
           [7.39436427e-06, 9.99992609e-01],
           [9.14310294e-05, 9.99908566e-01],
           [6.26367513e-10, 1.00000000e+00],
           [1.25905586e-07, 9.99999881e-01],
           [3.12472810e-04, 9.99687552e-01],
           [2.85553412e-08, 1.00000000e+00],
           [4.33354229e-01, 5.66645741e-01],
           [3.44269514e-01, 6.55730486e-01],
           [8.34467064e-04, 9.99165535e-01],
           [1.42052295e-05, 9.99985814e-01],
           [3.76239829e-02, 9.62375998e-01],
           [8.60990658e-02, 9.13900912e-01],
           [4.33692113e-02, 9.56630766e-01],
           [5.35908403e-05, 9.99946356e-01],
           [5.63074388e-02, 9.43692565e-01],
           [4.56764363e-02, 9.54323530e-01],
           [1.43748821e-06, 9.99998569e-01],
           [1.76271889e-04, 9.99823749e-01],
           [8.92129094e-07, 9.99999166e-01],
           [4.51967526e-06, 9.99995470e-01],
           [4.86808460e-07, 9.99999523e-01],
           [3.06162331e-03, 9.96938348e-01],
           [2.36557913e-03, 9.97634411e-01],
           [2.63574328e-02, 9.73642528e-01],
           [5.88492665e-04, 9.99411464e-01],
           [8.20539019e-04, 9.99179423e-01],
           [6.23777625e-04, 9.99376237e-01],
           [3.72303575e-02, 9.62769628e-01],
           [4.07574773e-02, 9.59242582e-01],
           [4.00654273e-03, 9.95993376e-01],
           [1.81943196e-05, 9.99981761e-01],
           [7.59859290e-07, 9.99999285e-01],
           [3.62041756e-04, 9.99637961e-01],
           [2.58774489e-05, 9.99974132e-01],
           [4.39166091e-02, 9.56083417e-01],
           [1.57068906e-04, 9.99842882e-01],
           [4.27546438e-05, 9.99957204e-01],
           [4.33851426e-07, 9.99999523e-01],
           [1.06258785e-05, 9.99989390e-01],
           [2.06652003e-05, 9.99979377e-01],
           [9.34310781e-04, 9.99065697e-01],
           [2.46719545e-04, 9.99753296e-01],
           [5.44905633e-05, 9.99945521e-01],
           [1.15819807e-06, 9.99998808e-01],
           [5.46740750e-07, 9.99999404e-01],
           [2.26472810e-04, 9.99773562e-01],
           [4.00237404e-02, 9.59976315e-01],
           [8.44577812e-08, 9.99999881e-01],
           [1.68859401e-06, 9.99998331e-01],
           [4.47045267e-02, 9.55295444e-01],
           [8.96677399e-10, 1.00000000e+00],
           [2.90987780e-04, 9.99708951e-01],
           [1.07738245e-02, 9.89226162e-01],
           [2.73737330e-02, 9.72626269e-01],
           [9.23420768e-04, 9.99076605e-01],
           [4.58183885e-03, 9.95418191e-01],
           [1.35699278e-02, 9.86430049e-01],
           [9.35107353e-04, 9.99064863e-01],
           [2.61069305e-04, 9.99738872e-01],
           [8.41016602e-03, 9.91589844e-01],
           [1.92446634e-03, 9.98075485e-01],
           [8.89113605e-01, 1.10886462e-01],
           [1.19203841e-02, 9.88079607e-01],
           [1.32559830e-08, 1.00000000e+00],
           [2.17384472e-03, 9.97826159e-01],
           [2.44831540e-06, 9.99997497e-01],
           [5.70836710e-04, 9.99429166e-01],
           [3.42735433e-07, 9.99999642e-01],
           [4.90733981e-01, 5.09266019e-01],
           [6.70888871e-02, 9.32911098e-01],
           [1.69997104e-02, 9.83000278e-01],
           [7.61657837e-04, 9.99238372e-01],
           [6.44089794e-03, 9.93559182e-01],
           [1.55331451e-03, 9.98446763e-01],
           [1.88843003e-08, 1.00000000e+00],
           [2.09901145e-07, 9.99999762e-01],
           [4.43483144e-02, 9.55651641e-01],
           [1.85570013e-06, 9.99998093e-01],
           [3.62450992e-05, 9.99963760e-01],
           [1.03086000e-04, 9.99896884e-01],
           [2.56862666e-04, 9.99743164e-01],
           [1.48763693e-05, 9.99985099e-01],
           [4.33091202e-07, 9.99999523e-01],
           [2.62359008e-02, 9.73764122e-01],
           [3.44101805e-04, 9.99655843e-01],
           [6.78985889e-05, 9.99932051e-01],
           [2.42761644e-06, 9.99997616e-01],
           [1.60705415e-04, 9.99839306e-01],
           [4.35274123e-05, 9.99956489e-01],
           [9.29921283e-04, 9.99070108e-01],
           [3.19567299e-03, 9.96804357e-01],
           [4.22352459e-04, 9.99577701e-01],
           [1.93878904e-07, 9.99999762e-01],
           [6.67742570e-05, 9.99933243e-01],
           [1.97356030e-07, 9.99999762e-01],
           [3.31358842e-06, 9.99996662e-01],
           [1.40250813e-05, 9.99985933e-01],
           [2.76338669e-05, 9.99972343e-01],
           [6.93335489e-04, 9.99306679e-01],
           [3.04071046e-03, 9.96959329e-01],
           [7.10827908e-06, 9.99992847e-01],
           [2.82081356e-03, 9.97179151e-01],
           [4.36793402e-04, 9.99563158e-01],
           [3.01161081e-01, 6.98838890e-01],
           [1.06907434e-08, 1.00000000e+00],
           [2.89844524e-04, 9.99710143e-01],
           [1.97238245e-04, 9.99802768e-01],
           [1.31387196e-05, 9.99986887e-01],
           [8.73077894e-08, 9.99999881e-01],
           [1.60455123e-01, 8.39544833e-01],
           [2.92100449e-04, 9.99707878e-01],
           [5.07528614e-03, 9.94924664e-01],
           [8.57472003e-07, 9.99999166e-01],
           [1.65325912e-06, 9.99998331e-01],
           [2.73302123e-02, 9.72669840e-01],
           [1.69914329e-05, 9.99982953e-01],
           [4.50420976e-02, 9.54957902e-01],
           [2.40814043e-05, 9.99975920e-01],
           [1.20851723e-07, 9.99999881e-01],
           [2.25539580e-02, 9.77446020e-01],
           [7.97868142e-06, 9.99992013e-01],
           [3.64887924e-08, 1.00000000e+00],
           [2.16053240e-03, 9.97839451e-01],
           [9.84864309e-04, 9.99015093e-01],
           [1.34959202e-02, 9.86504078e-01],
           [2.49141715e-02, 9.75085855e-01],
           [1.83328753e-03, 9.98166680e-01],
           [5.01486502e-05, 9.99949813e-01],
           [5.22246864e-03, 9.94777560e-01],
           [1.63405912e-05, 9.99983668e-01],
           [3.41849955e-05, 9.99965787e-01],
           [1.02724601e-02, 9.89727557e-01],
           [4.94979304e-06, 9.99994993e-01],
           [5.47747174e-03, 9.94522572e-01],
           [1.82228148e-01, 8.17771852e-01]], dtype=float32)

    lst=[]

    for i in range(0,len(y_pred)):
         k=np.argmax(y_pred[i]) #it gives index value of the highest probability for each iteration 
         print(k)
         lst.append(k)

    y_pred_label=np.array(lst)

    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1

    print(Y_train)

    [1 0 0 ... 0 1 1]

    np.unique(Y_train, return_counts=True)

    (array([0, 1]), array([ 734, 1174]))

    np.unique(Y_test, return_counts=True)

    (array([0, 1]), array([165, 313]))

    from sklearn.metrics import confusion_matrix
    from sklearn.metrics import plot_confusion_matrix

    mat = confusion_matrix(Y_test, y_pred_label) #we dont do this because we dont get the whole number on the confusion matrixis to fet the whole number annotation
    sns.heatmap(mat, square=True, annot=True, cbar=False)
    plt.xlabel('predicted value')
    plt.ylabel('true value');

[]

    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


    print('Accuracy: %.3f' % accuracy_score(y_true=Y_test, y_pred=y_pred_label))
    print('Precision: %.3f' % precision_score(y_true=Y_test, y_pred=y_pred_label))
    print('Recall: %.3f' % recall_score(y_true=Y_test, y_pred=y_pred_label))
    print('F1: %.3f' % f1_score(y_true=Y_test, y_pred=y_pred_label))

    Accuracy: 0.653
    Precision: 0.655
    Recall: 0.994
    F1: 0.789

    from sklearn import metrics

    # Model f1_score: how often is the classifier correct?
    Resnet_f1_score=metrics.f1_score(Y_test, y_pred_label)



    print("F1_score:",Resnet_f1_score)

    F1_score: 0.7893401015228427

    from sklearn.metrics import roc_curve
    from sklearn.metrics import auc
    fpr_keras, tpr_keras, thresholds_keras = roc_curve(Y_test, y_pred_label)


    auc_keras_ResNet50 = auc(fpr_keras, tpr_keras)
    auc_keras_ResNet50 #auc score

    0.4998354148513893

    import matplotlib.pyplot as plt
    plt.title('Resnet-Receiver Operating Characteristic')
    plt.plot(fpr_keras, tpr_keras, color='green',marker='o', label = 'AUC area = %0.2f' % auc_keras_ResNet50)
    plt.legend(loc = 'lower right')
    plt.plot([0, 1], [0, 1],'r--') #diagonal line
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')

    Text(0.5, 0, 'False Positive Rate')

[]

Model-InceptionV3

    from gc import callbacks
    from tensorflow.keras.applications.inception_v3 import InceptionV3 

    input_shape=(224,224,3)



    head_model = InceptionV3(include_top=False,
                     weights='imagenet',
                     #input_tensor=None,
                     input_shape=input_shape)
                     #pooling='avg',
                     #classes=2,
                     #classifier_activation='softmax')

    for layer in head_model.layers:
      layer.trainable = False #trainable are the last three layers until flatten (the whole set of fully connected layers)


    x = layers.Flatten()(head_model.output) #google: how to cut off a pre train model resnet and add fully connected layers in tensorflow
    x = layers.Dense(1000, activation='relu')(x)
    predictions = layers.Dense(2, activation = 'softmax')(x)

    model = Model(inputs = head_model.input, outputs = predictions)


    model.compile(optimizer=tf.keras.optimizers.Adam(0.00001),
                  loss=SparseCategoricalCrossentropy(from_logits=True),
                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])

    history=model.fit(
        X_train,Y_train,
        epochs=25, #can change the epoch
        validation_split=0.15, verbose=1,callbacks=[es])

    Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5
    87916544/87910968 [==============================] - 2s 0us/step
    87924736/87910968 [==============================] - 2s 0us/step
    Epoch 1/25

    /Users/camron/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: "`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?"
      return dispatch_target(*args, **kwargs)

    51/51 [==============================] - 45s 817ms/step - loss: 0.5329 - sparse_categorical_accuracy: 0.7606 - val_loss: 0.4614 - val_sparse_categorical_accuracy: 0.8223
    Epoch 2/25
    51/51 [==============================] - 43s 844ms/step - loss: 0.2209 - sparse_categorical_accuracy: 0.9161 - val_loss: 0.3155 - val_sparse_categorical_accuracy: 0.8676
    Epoch 3/25
    51/51 [==============================] - 42s 830ms/step - loss: 0.1357 - sparse_categorical_accuracy: 0.9574 - val_loss: 0.3186 - val_sparse_categorical_accuracy: 0.8537
    Epoch 4/25
    51/51 [==============================] - 42s 825ms/step - loss: 0.0721 - sparse_categorical_accuracy: 0.9895 - val_loss: 0.3219 - val_sparse_categorical_accuracy: 0.8641
    Epoch 5/25
    51/51 [==============================] - 42s 823ms/step - loss: 0.0486 - sparse_categorical_accuracy: 0.9981 - val_loss: 0.3196 - val_sparse_categorical_accuracy: 0.8606
    Epoch 6/25
    51/51 [==============================] - 41s 815ms/step - loss: 0.0341 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3250 - val_sparse_categorical_accuracy: 0.8676
    Epoch 7/25
    51/51 [==============================] - 42s 826ms/step - loss: 0.0281 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3321 - val_sparse_categorical_accuracy: 0.8537
    Epoch 8/25
    51/51 [==============================] - 42s 823ms/step - loss: 0.0211 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3331 - val_sparse_categorical_accuracy: 0.8606
    Epoch 9/25
    51/51 [==============================] - 42s 822ms/step - loss: 0.0174 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3373 - val_sparse_categorical_accuracy: 0.8641
    Epoch 10/25
    51/51 [==============================] - 42s 830ms/step - loss: 0.0143 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3390 - val_sparse_categorical_accuracy: 0.8606
    Epoch 11/25
    51/51 [==============================] - 42s 835ms/step - loss: 0.0124 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3428 - val_sparse_categorical_accuracy: 0.8571
    Epoch 12/25
    51/51 [==============================] - 42s 828ms/step - loss: 0.0110 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3515 - val_sparse_categorical_accuracy: 0.8606
    Epoch 13/25
    51/51 [==============================] - 42s 829ms/step - loss: 0.0093 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3500 - val_sparse_categorical_accuracy: 0.8641
    Epoch 14/25
    51/51 [==============================] - 42s 826ms/step - loss: 0.0082 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3569 - val_sparse_categorical_accuracy: 0.8606
    Epoch 15/25
    51/51 [==============================] - 42s 835ms/step - loss: 0.0072 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3546 - val_sparse_categorical_accuracy: 0.8676
    Epoch 16/25
    51/51 [==============================] - 42s 826ms/step - loss: 0.0066 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3602 - val_sparse_categorical_accuracy: 0.8676
    Epoch 17/25
    51/51 [==============================] - 42s 835ms/step - loss: 0.0059 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3631 - val_sparse_categorical_accuracy: 0.8606
    Epoch 18/25
    51/51 [==============================] - 43s 836ms/step - loss: 0.0054 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3661 - val_sparse_categorical_accuracy: 0.8641
    Epoch 19/25
    51/51 [==============================] - 42s 834ms/step - loss: 0.0049 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3730 - val_sparse_categorical_accuracy: 0.8711
    Epoch 20/25
    51/51 [==============================] - 42s 830ms/step - loss: 0.0044 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3835 - val_sparse_categorical_accuracy: 0.8537
    Epoch 21/25
    51/51 [==============================] - 42s 831ms/step - loss: 0.0041 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3792 - val_sparse_categorical_accuracy: 0.8641
    Epoch 22/25
    51/51 [==============================] - 42s 828ms/step - loss: 0.0038 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3749 - val_sparse_categorical_accuracy: 0.8606
    Epoch 23/25
    51/51 [==============================] - 42s 832ms/step - loss: 0.0035 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3760 - val_sparse_categorical_accuracy: 0.8606
    Epoch 24/25
    51/51 [==============================] - 44s 873ms/step - loss: 0.0032 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3848 - val_sparse_categorical_accuracy: 0.8641
    Epoch 25/25
    51/51 [==============================] - 43s 837ms/step - loss: 0.0029 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3870 - val_sparse_categorical_accuracy: 0.8606

    plt.plot(history.history['sparse_categorical_accuracy'])
    plt.plot(history.history['val_sparse_categorical_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()

[]

    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()

[]

    y_pred=model.predict(X_test) 
    y_pred

    array([[0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [3.0095157e-09, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00],
           [0.0000000e+00, 1.0000000e+00]], dtype=float32)

    lst=[]

    for i in range(0,len(y_pred)):
         k=np.argmax(y_pred[i]) #it gives index value of the highest probability for each iteration 
         print(k)
         lst.append(k)

    y_pred_label=np.array(lst)

    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1

    from sklearn.metrics import confusion_matrix
    from sklearn.metrics import plot_confusion_matrix

    mat = confusion_matrix(Y_test, y_pred_label) #we dont do this because we dont get the whole number on the confusion matrixis to fet the whole number annotation
    sns.heatmap(mat, square=True, annot=True, cbar=False)
    plt.xlabel('predicted value')
    plt.ylabel('true value');

[]

    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


    print('Accuracy: %.3f' % accuracy_score(y_true=Y_test, y_pred=y_pred_label))
    print('Precision: %.3f' % precision_score(y_true=Y_test, y_pred=y_pred_label))
    print('Recall: %.3f' % recall_score(y_true=Y_test, y_pred=y_pred_label))
    print('F1: %.3f' % f1_score(y_true=Y_test, y_pred=y_pred_label))

    Accuracy: 0.655
    Precision: 0.655
    Recall: 1.000
    F1: 0.791

    from sklearn import metrics

    # Model f1_score: how often is the classifier correct?
    InceptionV3_f1_score=metrics.f1_score(Y_test, y_pred_label)



    print("F1_score:",InceptionV3_f1_score)

    F1_score: 0.7914032869785081

    from sklearn.metrics import roc_curve
    from sklearn.metrics import auc
    fpr_keras, tpr_keras, thresholds_keras = roc_curve(Y_test, y_pred_label)


    auc_keras_InceptionV3 = auc(fpr_keras, tpr_keras)
    auc_keras_InceptionV3 #auc score

    0.5

    import matplotlib.pyplot as plt
    plt.title('InceptionV3-Receiver Operating Characteristic')
    plt.plot(fpr_keras, tpr_keras, color='green',marker='o', label = 'AUC area = %0.2f' % auc_keras_InceptionV3)
    plt.legend(loc = 'lower right')
    plt.plot([0, 1], [0, 1],'r--') #diagonal line
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')

    Text(0.5, 0, 'False Positive Rate')

[]

Model-VGG16

    from tensorflow.keras.applications.vgg16 import VGG16 

    input_shape=(224,224,3)



    head_model = VGG16(include_top=False,
                     weights='imagenet',
                     #input_tensor=None,
                     input_shape=input_shape)
                     #pooling='avg',
                     #classes=2,
                     #classifier_activation='softmax')


    for layer in head_model.layers:
      layer.trainable = False #trainable are the last three layers until flatten (the whole set of fully connected layers)


    x = layers.Flatten()(head_model.output) #google: how to cut off a pre train model resnet and add fully connected layers in tensorflow
    x = layers.Dense(1000, activation='relu')(x)
    predictions = layers.Dense(2, activation = 'softmax')(x)

    model = Model(inputs = head_model.input, outputs = predictions)




    model.compile(optimizer=tf.keras.optimizers.Adam(0.00001),
                  loss=SparseCategoricalCrossentropy(from_logits=True),
                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])

    history=model.fit(
        X_train,Y_train,
        epochs=25, #can change the epoch
        validation_split=0.15, verbose=1,callbacks=[es])

    Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5
    58892288/58889256 [==============================] - 1s 0us/step
    58900480/58889256 [==============================] - 1s 0us/step
    Epoch 1/25

    /Users/camron/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: "`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?"
      return dispatch_target(*args, **kwargs)

    51/51 [==============================] - 145s 3s/step - loss: 0.4684 - sparse_categorical_accuracy: 0.7989 - val_loss: 0.4094 - val_sparse_categorical_accuracy: 0.8328
    Epoch 2/25
    51/51 [==============================] - 142s 3s/step - loss: 0.3776 - sparse_categorical_accuracy: 0.8421 - val_loss: 0.3807 - val_sparse_categorical_accuracy: 0.8606
    Epoch 3/25
    51/51 [==============================] - 143s 3s/step - loss: 0.3385 - sparse_categorical_accuracy: 0.8637 - val_loss: 0.3601 - val_sparse_categorical_accuracy: 0.8676
    Epoch 4/25
    51/51 [==============================] - 144s 3s/step - loss: 0.3155 - sparse_categorical_accuracy: 0.8735 - val_loss: 0.3526 - val_sparse_categorical_accuracy: 0.8606
    Epoch 5/25
    51/51 [==============================] - 143s 3s/step - loss: 0.3004 - sparse_categorical_accuracy: 0.8742 - val_loss: 0.3397 - val_sparse_categorical_accuracy: 0.8641
    Epoch 6/25
    51/51 [==============================] - 142s 3s/step - loss: 0.2855 - sparse_categorical_accuracy: 0.8871 - val_loss: 0.3448 - val_sparse_categorical_accuracy: 0.8641
    Epoch 7/25
    51/51 [==============================] - 143s 3s/step - loss: 0.2843 - sparse_categorical_accuracy: 0.8834 - val_loss: 0.3300 - val_sparse_categorical_accuracy: 0.8641
    Epoch 8/25
    51/51 [==============================] - 142s 3s/step - loss: 0.2582 - sparse_categorical_accuracy: 0.8939 - val_loss: 0.3279 - val_sparse_categorical_accuracy: 0.8606
    Epoch 9/25
    51/51 [==============================] - 142s 3s/step - loss: 0.2514 - sparse_categorical_accuracy: 0.8970 - val_loss: 0.3223 - val_sparse_categorical_accuracy: 0.8676
    Epoch 10/25
    51/51 [==============================] - 141s 3s/step - loss: 0.2358 - sparse_categorical_accuracy: 0.9056 - val_loss: 0.3303 - val_sparse_categorical_accuracy: 0.8571
    Epoch 11/25
    51/51 [==============================] - 140s 3s/step - loss: 0.2299 - sparse_categorical_accuracy: 0.9025 - val_loss: 0.3201 - val_sparse_categorical_accuracy: 0.8641
    Epoch 12/25
    51/51 [==============================] - 139s 3s/step - loss: 0.2193 - sparse_categorical_accuracy: 0.9093 - val_loss: 0.3177 - val_sparse_categorical_accuracy: 0.8571
    Epoch 13/25
    51/51 [==============================] - 139s 3s/step - loss: 0.2112 - sparse_categorical_accuracy: 0.9112 - val_loss: 0.3190 - val_sparse_categorical_accuracy: 0.8711
    Epoch 14/25
    51/51 [==============================] - 138s 3s/step - loss: 0.2100 - sparse_categorical_accuracy: 0.9167 - val_loss: 0.3151 - val_sparse_categorical_accuracy: 0.8606
    Epoch 15/25
    51/51 [==============================] - 138s 3s/step - loss: 0.1994 - sparse_categorical_accuracy: 0.9241 - val_loss: 0.3128 - val_sparse_categorical_accuracy: 0.8711
    Epoch 16/25
    51/51 [==============================] - 149s 3s/step - loss: 0.1949 - sparse_categorical_accuracy: 0.9217 - val_loss: 0.3208 - val_sparse_categorical_accuracy: 0.8432
    Epoch 17/25
    51/51 [==============================] - 160s 3s/step - loss: 0.1940 - sparse_categorical_accuracy: 0.9284 - val_loss: 0.3114 - val_sparse_categorical_accuracy: 0.8571
    Epoch 18/25
    51/51 [==============================] - 161s 3s/step - loss: 0.1804 - sparse_categorical_accuracy: 0.9328 - val_loss: 0.3284 - val_sparse_categorical_accuracy: 0.8571
    Epoch 19/25
    51/51 [==============================] - 160s 3s/step - loss: 0.1755 - sparse_categorical_accuracy: 0.9371 - val_loss: 0.3298 - val_sparse_categorical_accuracy: 0.8676
    Epoch 20/25
    51/51 [==============================] - 161s 3s/step - loss: 0.1699 - sparse_categorical_accuracy: 0.9377 - val_loss: 0.3136 - val_sparse_categorical_accuracy: 0.8676
    Epoch 21/25
    51/51 [==============================] - 161s 3s/step - loss: 0.1633 - sparse_categorical_accuracy: 0.9383 - val_loss: 0.3179 - val_sparse_categorical_accuracy: 0.8711
    Epoch 22/25
    51/51 [==============================] - 161s 3s/step - loss: 0.1601 - sparse_categorical_accuracy: 0.9414 - val_loss: 0.3180 - val_sparse_categorical_accuracy: 0.8711
    Epoch 23/25
    51/51 [==============================] - 160s 3s/step - loss: 0.1555 - sparse_categorical_accuracy: 0.9445 - val_loss: 0.3134 - val_sparse_categorical_accuracy: 0.8571
    Epoch 24/25
    51/51 [==============================] - 164s 3s/step - loss: 0.1588 - sparse_categorical_accuracy: 0.9395 - val_loss: 0.3209 - val_sparse_categorical_accuracy: 0.8676
    Epoch 25/25
    51/51 [==============================] - 161s 3s/step - loss: 0.1483 - sparse_categorical_accuracy: 0.9463 - val_loss: 0.3152 - val_sparse_categorical_accuracy: 0.8606

    plt.plot(history.history['sparse_categorical_accuracy'])
    plt.plot(history.history['val_sparse_categorical_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()

[]

    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()

[]

    y_pred=model.predict(X_test) 
    y_pred

    array([[9.84251022e-01, 1.57489385e-02],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 3.74130102e-11],
           [9.99999762e-01, 2.49112901e-07],
           [6.09600700e-27, 1.00000000e+00],
           [1.70412920e-02, 9.82958734e-01],
           [3.94607894e-02, 9.60539222e-01],
           [0.00000000e+00, 1.00000000e+00],
           [4.71988682e-22, 1.00000000e+00],
           [3.79776478e-13, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [4.06633317e-02, 9.59336698e-01],
           [3.02168915e-11, 1.00000000e+00],
           [2.88459760e-11, 1.00000000e+00],
           [2.41196876e-08, 1.00000000e+00],
           [2.42922913e-21, 1.00000000e+00],
           [6.69695261e-24, 1.00000000e+00],
           [1.00000000e+00, 1.07313812e-15],
           [6.82117394e-13, 1.00000000e+00],
           [1.00000000e+00, 4.14388436e-17],
           [3.89092479e-14, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [2.36960803e-37, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [9.97949302e-01, 2.05066544e-03],
           [1.00000000e+00, 7.63577976e-14],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 1.85943113e-21],
           [0.00000000e+00, 1.00000000e+00],
           [3.10213417e-01, 6.89786613e-01],
           [6.72479824e-26, 1.00000000e+00],
           [2.23127883e-02, 9.77687180e-01],
           [5.14270737e-10, 1.00000000e+00],
           [2.70641709e-09, 1.00000000e+00],
           [1.08679131e-01, 8.91320944e-01],
           [3.21526397e-19, 1.00000000e+00],
           [2.02607126e-07, 9.99999762e-01],
           [1.72852404e-15, 1.00000000e+00],
           [9.43133169e-38, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [7.47560143e-01, 2.52439827e-01],
           [4.67829086e-05, 9.99953270e-01],
           [0.00000000e+00, 1.00000000e+00],
           [9.99800980e-01, 1.99064481e-04],
           [4.68063901e-14, 1.00000000e+00],
           [6.12110026e-13, 1.00000000e+00],
           [6.28136218e-01, 3.71863753e-01],
           [1.28818676e-03, 9.98711824e-01],
           [4.50560227e-02, 9.54943955e-01],
           [1.00000000e+00, 4.56419024e-24],
           [1.00000000e+00, 1.85491515e-13],
           [5.14231622e-02, 9.48576808e-01],
           [7.51788381e-22, 1.00000000e+00],
           [5.30454811e-21, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.69639215e-02, 9.83036101e-01],
           [4.95798669e-19, 1.00000000e+00],
           [9.22436438e-21, 1.00000000e+00],
           [1.58956487e-20, 1.00000000e+00],
           [2.15758123e-11, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [4.26130554e-11, 1.00000000e+00],
           [2.47349620e-01, 7.52650440e-01],
           [1.82635299e-16, 1.00000000e+00],
           [1.81504177e-07, 9.99999762e-01],
           [6.62538878e-05, 9.99933720e-01],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.84608636e-22, 1.00000000e+00],
           [4.46817023e-04, 9.99553144e-01],
           [0.00000000e+00, 1.00000000e+00],
           [1.44397110e-01, 8.55602920e-01],
           [9.99317408e-01, 6.82545418e-04],
           [9.99999762e-01, 2.54977891e-07],
           [0.00000000e+00, 1.00000000e+00],
           [1.70052735e-35, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [7.25341421e-22, 1.00000000e+00],
           [8.76833498e-03, 9.91231620e-01],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [8.06989222e-27, 1.00000000e+00],
           [9.51542437e-01, 4.84575555e-02],
           [4.51663993e-07, 9.99999523e-01],
           [1.41357616e-30, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [4.04610275e-16, 1.00000000e+00],
           [9.99609053e-01, 3.90928530e-04],
           [3.67654783e-17, 1.00000000e+00],
           [2.86387908e-03, 9.97136116e-01],
           [9.99589503e-01, 4.10481734e-04],
           [6.71390751e-07, 9.99999285e-01],
           [8.98260873e-21, 1.00000000e+00],
           [1.40194871e-07, 9.99999881e-01],
           [8.31524062e-07, 9.99999166e-01],
           [4.78434702e-15, 1.00000000e+00],
           [7.22449986e-15, 1.00000000e+00],
           [2.17778324e-06, 9.99997854e-01],
           [4.72503735e-15, 1.00000000e+00],
           [2.08873802e-24, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [2.78059247e-06, 9.99997258e-01],
           [1.00000000e+00, 1.74422671e-10],
           [1.19041671e-18, 1.00000000e+00],
           [1.57858793e-09, 1.00000000e+00],
           [1.00000000e+00, 5.16407056e-12],
           [0.00000000e+00, 1.00000000e+00],
           [6.10277292e-36, 1.00000000e+00],
           [1.00000000e+00, 7.98349067e-18],
           [2.15846995e-07, 9.99999762e-01],
           [8.24015737e-01, 1.75984249e-01],
           [0.00000000e+00, 1.00000000e+00],
           [9.99681354e-01, 3.18693696e-04],
           [1.00000000e+00, 9.47485033e-17],
           [1.00000000e+00, 4.56496785e-19],
           [9.99999881e-01, 1.16097112e-07],
           [5.43230870e-29, 1.00000000e+00],
           [9.99999642e-01, 3.58545009e-07],
           [2.25358008e-13, 1.00000000e+00],
           [8.77369344e-01, 1.22630686e-01],
           [1.00000000e+00, 2.19078089e-11],
           [1.00000000e+00, 1.72934842e-15],
           [1.00000000e+00, 3.68987498e-28],
           [0.00000000e+00, 1.00000000e+00],
           [2.68866263e-23, 1.00000000e+00],
           [6.50009491e-26, 1.00000000e+00],
           [7.49783590e-02, 9.25021648e-01],
           [2.52893171e-12, 1.00000000e+00],
           [9.52348560e-02, 9.04765189e-01],
           [4.47990561e-10, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [3.15396233e-21, 1.00000000e+00],
           [9.99980450e-01, 1.94982331e-05],
           [1.00000000e+00, 9.97362970e-10],
           [9.61817086e-01, 3.81829143e-02],
           [2.75255650e-01, 7.24744380e-01],
           [6.48470806e-13, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [8.78003844e-24, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.80382265e-31, 1.00000000e+00],
           [1.00000000e+00, 2.77535491e-15],
           [3.05573025e-07, 9.99999642e-01],
           [5.58190700e-18, 1.00000000e+00],
           [1.30872973e-25, 1.00000000e+00],
           [2.67696095e-07, 9.99999762e-01],
           [7.31072485e-01, 2.68927515e-01],
           [9.96264279e-01, 3.73576465e-03],
           [0.00000000e+00, 1.00000000e+00],
           [2.66809357e-17, 1.00000000e+00],
           [3.10857445e-01, 6.89142585e-01],
           [4.71336261e-06, 9.99995232e-01],
           [1.98647035e-15, 1.00000000e+00],
           [1.00000000e+00, 4.96339983e-11],
           [9.05072927e-01, 9.49270651e-02],
           [3.88506276e-04, 9.99611557e-01],
           [1.37595490e-17, 1.00000000e+00],
           [9.99999881e-01, 1.61500779e-07],
           [1.47084161e-28, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 4.94376777e-18],
           [5.74399460e-35, 1.00000000e+00],
           [3.09781911e-09, 1.00000000e+00],
           [9.51021314e-01, 4.89787348e-02],
           [1.90755134e-23, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [2.33790755e-01, 7.66209304e-01],
           [1.61547238e-08, 1.00000000e+00],
           [1.00000000e+00, 1.66180723e-08],
           [9.99989152e-01, 1.08162294e-05],
           [2.74630457e-18, 1.00000000e+00],
           [1.77955108e-18, 1.00000000e+00],
           [1.43793386e-18, 1.00000000e+00],
           [1.00000000e+00, 1.05115376e-23],
           [7.85730124e-01, 2.14269906e-01],
           [9.99916792e-01, 8.31818106e-05],
           [9.99999285e-01, 6.62876175e-07],
           [1.15278817e-05, 9.99988437e-01],
           [0.00000000e+00, 1.00000000e+00],
           [7.80919846e-03, 9.92190778e-01],
           [6.35088639e-08, 9.99999881e-01],
           [9.99999166e-01, 8.64414858e-07],
           [2.98733937e-07, 9.99999642e-01],
           [2.69326210e-06, 9.99997258e-01],
           [2.69310030e-28, 1.00000000e+00],
           [6.35069242e-08, 9.99999881e-01],
           [2.24539646e-18, 1.00000000e+00],
           [1.00000000e+00, 8.05647904e-09],
           [9.90472078e-01, 9.52793006e-03],
           [0.00000000e+00, 1.00000000e+00],
           [8.09661505e-11, 1.00000000e+00],
           [1.00000000e+00, 5.71989034e-08],
           [5.26751755e-05, 9.99947309e-01],
           [1.87778394e-38, 1.00000000e+00],
           [9.99986768e-01, 1.32689947e-05],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [7.23663799e-37, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [9.75168025e-29, 1.00000000e+00],
           [1.00000000e+00, 1.65414602e-29],
           [4.96164933e-02, 9.50383544e-01],
           [6.65821731e-10, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [8.66798146e-05, 9.99913335e-01],
           [1.00000000e+00, 1.31691734e-33],
           [1.24197849e-03, 9.98758078e-01],
           [2.01969795e-18, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.72084359e-18, 1.00000000e+00],
           [3.46833692e-15, 1.00000000e+00],
           [8.76002684e-02, 9.12399769e-01],
           [8.84907240e-06, 9.99991179e-01],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [4.22852254e-03, 9.95771468e-01],
           [2.33391509e-27, 1.00000000e+00],
           [1.92688808e-01, 8.07311177e-01],
           [1.02147500e-23, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 9.23261398e-11],
           [1.96874450e-09, 1.00000000e+00],
           [1.95942665e-10, 1.00000000e+00],
           [9.99995947e-01, 4.00325735e-06],
           [1.35222763e-05, 9.99986529e-01],
           [1.90957777e-13, 1.00000000e+00],
           [4.48728418e-22, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [9.30647013e-21, 1.00000000e+00],
           [9.96342480e-01, 3.65744601e-03],
           [1.93570557e-07, 9.99999762e-01],
           [9.88487363e-01, 1.15126586e-02],
           [1.64602986e-17, 1.00000000e+00],
           [7.40215560e-20, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [2.78877287e-20, 1.00000000e+00],
           [1.50488336e-06, 9.99998450e-01],
           [1.00000000e+00, 2.23215246e-09],
           [2.46226539e-10, 1.00000000e+00],
           [1.00000000e+00, 2.95990217e-36],
           [1.00000000e+00, 2.57418691e-15],
           [3.10933315e-20, 1.00000000e+00],
           [7.04865519e-19, 1.00000000e+00],
           [9.70527435e-06, 9.99990344e-01],
           [1.00000000e+00, 6.58892502e-32],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 2.08807283e-09],
           [9.99920607e-01, 7.93944419e-05],
           [1.00000000e+00, 1.18522578e-15],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 1.89759373e-08],
           [1.00000000e+00, 2.13837801e-08],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 9.13586359e-15],
           [6.41695966e-14, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.52502731e-30, 1.00000000e+00],
           [1.78005041e-10, 1.00000000e+00],
           [1.38571271e-21, 1.00000000e+00],
           [9.99725163e-01, 2.74873426e-04],
           [1.43418320e-23, 1.00000000e+00],
           [6.36005234e-15, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [2.05758020e-38, 1.00000000e+00],
           [9.99998450e-01, 1.58529542e-06],
           [1.00000000e+00, 6.04646599e-09],
           [2.55981516e-35, 1.00000000e+00],
           [9.99999881e-01, 8.87023077e-08],
           [1.00000000e+00, 6.31982819e-24],
           [8.82569225e-07, 9.99999166e-01],
           [1.81624119e-03, 9.98183787e-01],
           [4.70562046e-03, 9.95294392e-01],
           [1.65452601e-20, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [3.22013297e-14, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.03873407e-21, 1.00000000e+00],
           [1.00000000e+00, 4.98758410e-17],
           [2.30177397e-28, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [8.20755653e-18, 1.00000000e+00],
           [1.59494618e-09, 1.00000000e+00],
           [9.99998689e-01, 1.27665828e-06],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [2.72272057e-32, 1.00000000e+00],
           [1.16627316e-13, 1.00000000e+00],
           [9.99994993e-01, 5.02725197e-06],
           [1.43255409e-30, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [7.29513942e-17, 1.00000000e+00],
           [6.27523812e-04, 9.99372542e-01],
           [2.02342712e-10, 1.00000000e+00],
           [2.65744433e-35, 1.00000000e+00],
           [3.38985547e-22, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [9.99874592e-01, 1.25438673e-04],
           [3.74306366e-02, 9.62569356e-01],
           [2.77735040e-12, 1.00000000e+00],
           [5.23716665e-07, 9.99999523e-01],
           [9.99999166e-01, 8.66376581e-07],
           [7.98854232e-01, 2.01145709e-01],
           [3.79669927e-07, 9.99999642e-01],
           [5.77990086e-12, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [8.53204529e-06, 9.99991417e-01],
           [1.20389764e-03, 9.98796105e-01],
           [2.50223063e-12, 1.00000000e+00],
           [7.81235970e-13, 1.00000000e+00],
           [9.99978065e-01, 2.19078756e-05],
           [1.78909187e-09, 1.00000000e+00],
           [4.21661405e-17, 1.00000000e+00],
           [3.69303575e-06, 9.99996305e-01],
           [4.89015207e-02, 9.51098502e-01],
           [1.00000000e+00, 1.09372732e-16],
           [6.04923515e-20, 1.00000000e+00],
           [9.79527533e-01, 2.04725340e-02],
           [4.52948836e-22, 1.00000000e+00],
           [2.31180491e-10, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.09659159e-05, 9.99989033e-01],
           [1.21465251e-01, 8.78534734e-01],
           [0.00000000e+00, 1.00000000e+00],
           [9.99942660e-01, 5.73186298e-05],
           [1.75231889e-07, 9.99999881e-01],
           [8.34024405e-10, 1.00000000e+00],
           [4.03048628e-29, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [4.63136762e-30, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [9.84991372e-01, 1.50086926e-02],
           [9.99818146e-01, 1.81909869e-04],
           [1.97257009e-21, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [5.65406343e-04, 9.99434531e-01],
           [9.99999285e-01, 7.39473478e-07],
           [3.35895429e-19, 1.00000000e+00],
           [1.00000000e+00, 6.52268668e-22],
           [1.00000000e+00, 3.49940330e-18],
           [6.91967007e-15, 1.00000000e+00],
           [6.49053700e-09, 1.00000000e+00],
           [8.32745168e-13, 1.00000000e+00],
           [1.81085120e-35, 1.00000000e+00],
           [1.14795872e-11, 1.00000000e+00],
           [2.05294182e-09, 1.00000000e+00],
           [1.00000000e+00, 9.61207441e-29],
           [4.12656888e-11, 1.00000000e+00],
           [9.99996185e-01, 3.80123129e-06],
           [0.00000000e+00, 1.00000000e+00],
           [1.38885827e-18, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.44998140e-07, 9.99999881e-01],
           [9.99655962e-01, 3.44047032e-04],
           [0.00000000e+00, 1.00000000e+00],
           [9.99915123e-01, 8.48730197e-05],
           [6.80800021e-15, 1.00000000e+00],
           [9.99510050e-01, 4.89933707e-04],
           [1.32447753e-36, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [7.39514580e-05, 9.99926090e-01],
           [7.14848968e-20, 1.00000000e+00],
           [8.79829038e-36, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [9.86337125e-01, 1.36628421e-02],
           [9.86431539e-01, 1.35685056e-02],
           [6.67574433e-20, 1.00000000e+00],
           [9.21201439e-15, 1.00000000e+00],
           [7.02592899e-07, 9.99999285e-01],
           [2.39716068e-01, 7.60283947e-01],
           [1.00000000e+00, 4.92630146e-24],
           [0.00000000e+00, 1.00000000e+00],
           [2.42337165e-20, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [9.99129832e-01, 8.70191667e-04],
           [1.00000000e+00, 2.14253039e-19],
           [1.00000000e+00, 3.15648369e-10],
           [0.00000000e+00, 1.00000000e+00],
           [3.18708981e-13, 1.00000000e+00],
           [8.16293955e-01, 1.83706075e-01],
           [9.99972939e-01, 2.71125791e-05],
           [1.00000000e+00, 1.03861732e-17],
           [8.02609616e-16, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [9.99999642e-01, 3.56363643e-07],
           [9.99999762e-01, 2.21481287e-07],
           [0.00000000e+00, 1.00000000e+00],
           [7.96172325e-16, 1.00000000e+00],
           [1.00000000e+00, 9.41685152e-14],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 2.66607485e-35],
           [9.99985695e-01, 1.42869485e-05],
           [1.52005062e-01, 8.47994864e-01],
           [1.00000000e+00, 4.12412593e-09],
           [1.00000000e+00, 7.11373044e-11],
           [5.94967414e-23, 1.00000000e+00],
           [9.99999404e-01, 6.38374331e-07],
           [1.57975517e-02, 9.84202385e-01],
           [6.99064196e-30, 1.00000000e+00],
           [6.14826163e-08, 9.99999881e-01],
           [7.80044364e-14, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00584817e-04, 9.99899387e-01],
           [9.99786079e-01, 2.13977721e-04],
           [6.70647656e-04, 9.99329329e-01],
           [0.00000000e+00, 1.00000000e+00],
           [2.33384576e-02, 9.76661503e-01],
           [1.83402211e-20, 1.00000000e+00],
           [9.97902065e-35, 1.00000000e+00],
           [5.60048292e-11, 1.00000000e+00],
           [1.84251977e-14, 1.00000000e+00],
           [4.28423859e-11, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00050466e-28, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [3.38437443e-04, 9.99661565e-01],
           [1.11958203e-18, 1.00000000e+00],
           [5.95570683e-01, 4.04429346e-01],
           [9.99998927e-01, 1.05594233e-06],
           [1.26351485e-09, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [6.19247931e-19, 1.00000000e+00],
           [1.09608025e-21, 1.00000000e+00],
           [1.00000000e+00, 8.02320450e-12],
           [0.00000000e+00, 1.00000000e+00],
           [1.00000000e+00, 1.14593908e-08],
           [1.25276460e-06, 9.99998689e-01],
           [9.95717108e-01, 4.28284006e-03],
           [7.25974003e-03, 9.92740333e-01],
           [1.00000000e+00, 9.21424505e-19],
           [7.00429928e-06, 9.99992967e-01],
           [0.00000000e+00, 1.00000000e+00],
           [1.88452438e-01, 8.11547518e-01],
           [9.99996543e-01, 3.48966455e-06],
           [1.00000000e+00, 2.41154861e-13],
           [1.00000000e+00, 9.80351289e-24],
           [0.00000000e+00, 1.00000000e+00],
           [6.78850309e-10, 1.00000000e+00],
           [3.06204839e-10, 1.00000000e+00],
           [1.63873727e-17, 1.00000000e+00],
           [9.99994159e-01, 5.80966071e-06],
           [6.36324095e-27, 1.00000000e+00],
           [3.98720196e-10, 1.00000000e+00],
           [3.07908684e-01, 6.92091286e-01],
           [0.00000000e+00, 1.00000000e+00],
           [8.42190068e-03, 9.91578102e-01],
           [1.00000000e+00, 1.39927429e-28],
           [4.98347690e-05, 9.99950171e-01],
           [1.00000000e+00, 2.79506152e-09],
           [0.00000000e+00, 1.00000000e+00],
           [9.99999046e-01, 9.36059223e-07],
           [0.00000000e+00, 1.00000000e+00],
           [8.53259407e-24, 1.00000000e+00],
           [6.86426237e-02, 9.31357324e-01],
           [4.12567550e-13, 1.00000000e+00],
           [1.01683051e-09, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.04467843e-04, 9.99895573e-01],
           [6.05459730e-18, 1.00000000e+00],
           [1.02088308e-30, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [2.76936847e-23, 1.00000000e+00],
           [9.99945283e-01, 5.47138879e-05],
           [9.99996781e-01, 3.24403004e-06],
           [1.63829513e-15, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [4.21196472e-26, 1.00000000e+00],
           [9.99999404e-01, 6.37429594e-07]], dtype=float32)

    lst=[]

    for i in range(0,len(y_pred)):
         k=np.argmax(y_pred[i]) #it gives index value of the highest probability for each iteration 
         print(k)
         lst.append(k)

    y_pred_label=np.array(lst)

    0
    1
    1
    0
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    0
    1
    1
    1
    1
    0
    0
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    0
    1
    1
    0
    1
    1
    0
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    0
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    0
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    0
    1
    1
    0
    1
    0
    1
    0
    0
    0
    0
    1
    0
    1
    0
    0
    0
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    0
    0
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    0
    0
    1
    1
    1
    1
    1
    0
    0
    1
    1
    0
    1
    1
    0
    1
    1
    0
    1
    1
    1
    1
    0
    0
    1
    1
    1
    0
    0
    0
    0
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    0
    0
    1
    1
    0
    1
    1
    0
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    0
    1
    1
    1
    1
    1
    1
    0
    1
    0
    1
    1
    1
    1
    1
    0
    1
    0
    0
    1
    1
    1
    0
    1
    0
    0
    0
    1
    0
    0
    1
    0
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    0
    0
    1
    0
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    0
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    0
    0
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    0
    1
    0
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    0
    0
    1
    1
    1
    0
    1
    0
    0
    1
    1
    1
    1
    1
    1
    0
    1
    0
    1
    1
    1
    1
    0
    1
    0
    1
    0
    1
    1
    1
    1
    1
    1
    0
    0
    1
    1
    1
    1
    0
    1
    1
    1
    0
    0
    0
    1
    1
    0
    0
    0
    1
    1
    0
    0
    1
    1
    0
    1
    0
    0
    1
    0
    0
    1
    0
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    0
    1
    1
    1
    1
    0
    1
    0
    1
    0
    1
    0
    1
    1
    1
    0
    0
    0
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    0
    1
    0
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    0
    1
    1
    1
    1
    1
    1
    0

    from sklearn.metrics import confusion_matrix
    from sklearn.metrics import plot_confusion_matrix

    mat = confusion_matrix(Y_test, y_pred_label) #we dont do this because we dont get the whole number on the confusion matrixis to fet the whole number annotation
    sns.heatmap(mat, square=True, annot=True, cbar=False)
    plt.xlabel('predicted value')
    plt.ylabel('true value');

[]

    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


    print('Accuracy: %.3f' % accuracy_score(y_true=Y_test, y_pred=y_pred_label))
    print('Precision: %.3f' % precision_score(y_true=Y_test, y_pred=y_pred_label))
    print('Recall: %.3f' % recall_score(y_true=Y_test, y_pred=y_pred_label))
    print('F1: %.3f' % f1_score(y_true=Y_test, y_pred=y_pred_label))

    Accuracy: 0.785
    Precision: 0.800
    Recall: 0.895
    F1: 0.845

    from sklearn import metrics

    # Model f1_score: how often is the classifier correct?
    VGG16_f1_score=metrics.f1_score(Y_test, y_pred_label)



    print("F1_score:",VGG16_f1_score)

    F1_score: 0.8446455505279035

    from sklearn.metrics import roc_curve
    from sklearn.metrics import auc
    fpr_keras, tpr_keras, thresholds_keras = roc_curve(Y_test, y_pred_label)


    auc_keras_VGG16 = auc(fpr_keras, tpr_keras)
    auc_keras_VGG16 #auc score

    0.7351631329267111

    import matplotlib.pyplot as plt
    plt.title('VGG16-Receiver Operating Characteristic')
    plt.plot(fpr_keras, tpr_keras, color='green',marker='o', label = 'AUC area = %0.2f' % auc_keras_VGG16)
    plt.legend(loc = 'lower right')
    plt.plot([0, 1], [0, 1],'r--') #diagonal line
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')

    Text(0.5, 0, 'False Positive Rate')

[]

Model-DenseNet121

    from tensorflow.keras.applications.densenet import DenseNet121

    input_shape=(224,224,3)



    head_model = DenseNet121(include_top=False,
                     weights='imagenet',
                     #input_tensor=None,
                     input_shape=input_shape)
                     #pooling='avg',
                     #classes=2)

    for layer in head_model.layers:
      layer.trainable = False #trainable are the last three layers until flatten (the whole set of fully connected layers)


    x = layers.Flatten()(head_model.output) #google: how to cut off a pre train model resnet and add fully connected layers in tensorflow
    x = layers.Dense(1000, activation='relu')(x)
    predictions = layers.Dense(2, activation = 'softmax')(x)

    model = Model(inputs = head_model.input, outputs = predictions)






    model.compile(optimizer=tf.keras.optimizers.Adam(0.00001),
                  loss=SparseCategoricalCrossentropy(from_logits=True),
                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])

    history=model.fit(
        X_train,Y_train,
        epochs=25, #can change the epoch
        validation_split=0.15, verbose=1,callbacks=[es])

    Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5
    29089792/29084464 [==============================] - 1s 0us/step
    29097984/29084464 [==============================] - 1s 0us/step
    Epoch 1/25

    /Users/camron/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: "`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?"
      return dispatch_target(*args, **kwargs)

    51/51 [==============================] - 145s 3s/step - loss: 0.4580 - sparse_categorical_accuracy: 0.8051 - val_loss: 0.3654 - val_sparse_categorical_accuracy: 0.8397
    Epoch 2/25
    51/51 [==============================] - 142s 3s/step - loss: 0.2108 - sparse_categorical_accuracy: 0.9204 - val_loss: 0.3480 - val_sparse_categorical_accuracy: 0.8432
    Epoch 3/25
    51/51 [==============================] - 142s 3s/step - loss: 0.1305 - sparse_categorical_accuracy: 0.9537 - val_loss: 0.2946 - val_sparse_categorical_accuracy: 0.8711
    Epoch 4/25
    51/51 [==============================] - 142s 3s/step - loss: 0.0831 - sparse_categorical_accuracy: 0.9827 - val_loss: 0.2932 - val_sparse_categorical_accuracy: 0.8780
    Epoch 5/25
    51/51 [==============================] - 142s 3s/step - loss: 0.0611 - sparse_categorical_accuracy: 0.9907 - val_loss: 0.2918 - val_sparse_categorical_accuracy: 0.8780
    Epoch 6/25
    51/51 [==============================] - 142s 3s/step - loss: 0.0459 - sparse_categorical_accuracy: 0.9975 - val_loss: 0.2844 - val_sparse_categorical_accuracy: 0.8920
    Epoch 7/25
    51/51 [==============================] - 141s 3s/step - loss: 0.0359 - sparse_categorical_accuracy: 0.9975 - val_loss: 0.2868 - val_sparse_categorical_accuracy: 0.8850
    Epoch 8/25
    51/51 [==============================] - 142s 3s/step - loss: 0.0282 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2910 - val_sparse_categorical_accuracy: 0.8885
    Epoch 9/25
    51/51 [==============================] - 141s 3s/step - loss: 0.0238 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2983 - val_sparse_categorical_accuracy: 0.8780
    Epoch 10/25
    51/51 [==============================] - 142s 3s/step - loss: 0.0207 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2953 - val_sparse_categorical_accuracy: 0.8780
    Epoch 11/25
    51/51 [==============================] - 142s 3s/step - loss: 0.0171 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3085 - val_sparse_categorical_accuracy: 0.8746
    Epoch 12/25
    51/51 [==============================] - 142s 3s/step - loss: 0.0144 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2988 - val_sparse_categorical_accuracy: 0.8815
    Epoch 13/25
    51/51 [==============================] - 142s 3s/step - loss: 0.0127 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3009 - val_sparse_categorical_accuracy: 0.8780
    Epoch 14/25
    51/51 [==============================] - 142s 3s/step - loss: 0.0110 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3021 - val_sparse_categorical_accuracy: 0.8850
    Epoch 15/25
    51/51 [==============================] - 142s 3s/step - loss: 0.0098 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3120 - val_sparse_categorical_accuracy: 0.8885
    Epoch 16/25
    51/51 [==============================] - 142s 3s/step - loss: 0.0088 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3050 - val_sparse_categorical_accuracy: 0.8850
    Epoch 17/25
    51/51 [==============================] - 127s 2s/step - loss: 0.0080 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3123 - val_sparse_categorical_accuracy: 0.8850
    Epoch 18/25
    51/51 [==============================] - 129s 3s/step - loss: 0.0071 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3098 - val_sparse_categorical_accuracy: 0.8815
    Epoch 19/25
    51/51 [==============================] - 130s 3s/step - loss: 0.0066 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3143 - val_sparse_categorical_accuracy: 0.8850
    Epoch 20/25
    51/51 [==============================] - 129s 3s/step - loss: 0.0059 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3170 - val_sparse_categorical_accuracy: 0.8850
    Epoch 21/25
    51/51 [==============================] - 129s 3s/step - loss: 0.0055 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3182 - val_sparse_categorical_accuracy: 0.8850
    Epoch 22/25
    51/51 [==============================] - 128s 3s/step - loss: 0.0051 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3204 - val_sparse_categorical_accuracy: 0.8920
    Epoch 23/25
    51/51 [==============================] - 129s 3s/step - loss: 0.0046 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3145 - val_sparse_categorical_accuracy: 0.8850
    Epoch 24/25
    51/51 [==============================] - 130s 3s/step - loss: 0.0043 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3177 - val_sparse_categorical_accuracy: 0.8850
    Epoch 25/25
    51/51 [==============================] - 134s 3s/step - loss: 0.0041 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3265 - val_sparse_categorical_accuracy: 0.8885

    plt.plot(history.history['sparse_categorical_accuracy'])
    plt.plot(history.history['val_sparse_categorical_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()

[]

    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()

[]

    y_pred=model.predict(X_test) 
    y_pred

    array([[1.11260082e-31, 1.00000000e+00],
           [3.32149260e-12, 1.00000000e+00],
           [5.44452945e-21, 1.00000000e+00],
           [9.99992728e-01, 7.24281654e-06],
           [9.99144793e-01, 8.55189748e-04],
           [5.52159607e-09, 1.00000000e+00],
           [9.94700687e-30, 1.00000000e+00],
           [4.88740559e-09, 1.00000000e+00],
           [1.03768326e-16, 1.00000000e+00],
           [1.90303871e-22, 1.00000000e+00],
           [8.54952782e-02, 9.14504707e-01],
           [2.01039642e-04, 9.99798954e-01],
           [9.67501998e-01, 3.24980319e-02],
           [5.51071402e-14, 1.00000000e+00],
           [9.98777092e-01, 1.22290698e-03],
           [7.80439677e-06, 9.99992251e-01],
           [7.05384551e-08, 9.99999881e-01],
           [1.44947446e-14, 1.00000000e+00],
           [1.15894983e-02, 9.88410473e-01],
           [1.00052739e-06, 9.99999046e-01],
           [0.00000000e+00, 1.00000000e+00],
           [3.94775185e-11, 1.00000000e+00],
           [1.37628039e-17, 1.00000000e+00],
           [1.59108142e-20, 1.00000000e+00],
           [3.19369114e-10, 1.00000000e+00],
           [7.97591913e-31, 1.00000000e+00],
           [3.01497286e-32, 1.00000000e+00],
           [4.74667735e-03, 9.95253325e-01],
           [5.65507343e-27, 1.00000000e+00],
           [1.71214965e-16, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [9.12350661e-05, 9.99908805e-01],
           [1.78166081e-21, 1.00000000e+00],
           [8.98282143e-12, 1.00000000e+00],
           [5.22808270e-16, 1.00000000e+00],
           [8.65203276e-07, 9.99999166e-01],
           [1.96925043e-06, 9.99997973e-01],
           [1.28419109e-04, 9.99871612e-01],
           [1.43594002e-16, 1.00000000e+00],
           [2.57592680e-14, 1.00000000e+00],
           [9.02136212e-17, 1.00000000e+00],
           [1.17164950e-06, 9.99998808e-01],
           [5.22665857e-19, 1.00000000e+00],
           [1.31222737e-20, 1.00000000e+00],
           [8.50373258e-33, 1.00000000e+00],
           [3.56419450e-09, 1.00000000e+00],
           [5.99147513e-28, 1.00000000e+00],
           [7.53710410e-08, 9.99999881e-01],
           [1.26600426e-04, 9.99873400e-01],
           [1.52025209e-16, 1.00000000e+00],
           [9.99532700e-01, 4.67316248e-04],
           [0.00000000e+00, 1.00000000e+00],
           [3.14404289e-08, 1.00000000e+00],
           [2.55911015e-14, 1.00000000e+00],
           [6.83616323e-04, 9.99316335e-01],
           [1.45230981e-04, 9.99854803e-01],
           [1.20715382e-10, 1.00000000e+00],
           [7.54112698e-05, 9.99924541e-01],
           [1.14618579e-16, 1.00000000e+00],
           [6.40534456e-21, 1.00000000e+00],
           [1.27296622e-12, 1.00000000e+00],
           [1.86207675e-04, 9.99813855e-01],
           [5.50237359e-18, 1.00000000e+00],
           [8.56323297e-08, 9.99999881e-01],
           [3.85952355e-08, 1.00000000e+00],
           [1.57635217e-13, 1.00000000e+00],
           [6.58129142e-16, 1.00000000e+00],
           [4.80085117e-32, 1.00000000e+00],
           [1.53896970e-17, 1.00000000e+00],
           [2.50269414e-19, 1.00000000e+00],
           [2.39209975e-26, 1.00000000e+00],
           [9.99273360e-01, 7.26592436e-04],
           [1.43622933e-10, 1.00000000e+00],
           [7.41206751e-14, 1.00000000e+00],
           [9.08515363e-16, 1.00000000e+00],
           [5.53838456e-14, 1.00000000e+00],
           [1.18270309e-08, 1.00000000e+00],
           [6.26166535e-22, 1.00000000e+00],
           [4.62911665e-01, 5.37088335e-01],
           [1.00000000e+00, 1.53492543e-08],
           [9.41063430e-15, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.46915074e-02, 9.85308468e-01],
           [2.34211300e-04, 9.99765813e-01],
           [8.03681850e-01, 1.96318090e-01],
           [3.53241994e-05, 9.99964714e-01],
           [6.30908411e-13, 1.00000000e+00],
           [3.19648835e-10, 1.00000000e+00],
           [5.30057287e-25, 1.00000000e+00],
           [4.30459386e-06, 9.99995708e-01],
           [8.56834714e-23, 1.00000000e+00],
           [2.18382821e-13, 1.00000000e+00],
           [1.33414878e-27, 1.00000000e+00],
           [1.22022266e-06, 9.99998808e-01],
           [1.63971166e-07, 9.99999881e-01],
           [4.24188256e-05, 9.99957561e-01],
           [1.27216817e-05, 9.99987245e-01],
           [5.43715180e-17, 1.00000000e+00],
           [4.68865008e-12, 1.00000000e+00],
           [1.71102816e-04, 9.99828815e-01],
           [3.18350832e-11, 1.00000000e+00],
           [5.57305617e-18, 1.00000000e+00],
           [1.03462869e-22, 1.00000000e+00],
           [1.05121771e-12, 1.00000000e+00],
           [7.67480662e-11, 1.00000000e+00],
           [1.80502777e-08, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.27767085e-03, 9.98722255e-01],
           [2.12428287e-13, 1.00000000e+00],
           [5.74605213e-21, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [3.26639040e-37, 1.00000000e+00],
           [5.25218844e-01, 4.74781215e-01],
           [4.74099684e-38, 1.00000000e+00],
           [3.70708563e-29, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [4.82071132e-23, 1.00000000e+00],
           [5.05325124e-06, 9.99994993e-01],
           [1.84625569e-14, 1.00000000e+00],
           [9.26195565e-09, 1.00000000e+00],
           [2.71526240e-02, 9.72847342e-01],
           [1.01028398e-14, 1.00000000e+00],
           [1.52243175e-34, 1.00000000e+00],
           [5.82207936e-23, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [4.29737046e-09, 1.00000000e+00],
           [1.23648186e-20, 1.00000000e+00],
           [7.59252544e-06, 9.99992371e-01],
           [2.62969563e-10, 1.00000000e+00],
           [1.30424651e-14, 1.00000000e+00],
           [1.72580648e-11, 1.00000000e+00],
           [2.39168541e-29, 1.00000000e+00],
           [9.99999881e-01, 7.35769206e-08],
           [6.28366888e-01, 3.71633112e-01],
           [1.82367937e-24, 1.00000000e+00],
           [4.42036201e-22, 1.00000000e+00],
           [9.86018121e-32, 1.00000000e+00],
           [5.11877513e-38, 1.00000000e+00],
           [7.73976790e-05, 9.99922633e-01],
           [1.83349740e-35, 1.00000000e+00],
           [1.30892605e-15, 1.00000000e+00],
           [9.30397093e-09, 1.00000000e+00],
           [1.33574729e-09, 1.00000000e+00],
           [1.49054598e-24, 1.00000000e+00],
           [9.99953747e-01, 4.61939962e-05],
           [1.45945227e-19, 1.00000000e+00],
           [8.57930863e-05, 9.99914169e-01],
           [1.59502311e-23, 1.00000000e+00],
           [3.12961038e-20, 1.00000000e+00],
           [4.48711404e-29, 1.00000000e+00],
           [1.03713944e-08, 1.00000000e+00],
           [6.51149100e-17, 1.00000000e+00],
           [3.45165300e-23, 1.00000000e+00],
           [1.29900360e-03, 9.98700976e-01],
           [2.12440136e-06, 9.99997854e-01],
           [2.84539303e-09, 1.00000000e+00],
           [1.07110680e-20, 1.00000000e+00],
           [9.94119525e-01, 5.88047784e-03],
           [5.22999075e-11, 1.00000000e+00],
           [1.90057054e-16, 1.00000000e+00],
           [9.45091702e-17, 1.00000000e+00],
           [1.97247076e-07, 9.99999762e-01],
           [6.43000216e-27, 1.00000000e+00],
           [2.33037723e-17, 1.00000000e+00],
           [5.46086223e-12, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [8.03838152e-09, 1.00000000e+00],
           [2.49499393e-10, 1.00000000e+00],
           [1.76153183e-02, 9.82384622e-01],
           [9.10062850e-01, 8.99371952e-02],
           [1.35700255e-31, 1.00000000e+00],
           [1.61292817e-28, 1.00000000e+00],
           [3.92238802e-12, 1.00000000e+00],
           [1.52275517e-14, 1.00000000e+00],
           [7.53826380e-01, 2.46173605e-01],
           [3.21964827e-25, 1.00000000e+00],
           [3.96080181e-36, 1.00000000e+00],
           [1.56535675e-20, 1.00000000e+00],
           [3.84628378e-14, 1.00000000e+00],
           [5.17514422e-08, 1.00000000e+00],
           [8.95544520e-28, 1.00000000e+00],
           [6.74109518e-11, 1.00000000e+00],
           [3.13047813e-15, 1.00000000e+00],
           [5.45058946e-24, 1.00000000e+00],
           [3.75067043e-06, 9.99996305e-01],
           [7.64417284e-14, 1.00000000e+00],
           [6.56529009e-01, 3.43470961e-01],
           [3.40026167e-11, 1.00000000e+00],
           [1.36324652e-09, 1.00000000e+00],
           [2.22417898e-17, 1.00000000e+00],
           [2.13154831e-20, 1.00000000e+00],
           [3.21025783e-15, 1.00000000e+00],
           [5.49041317e-04, 9.99450982e-01],
           [1.64149662e-06, 9.99998331e-01],
           [8.38471292e-10, 1.00000000e+00],
           [2.39080138e-21, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [3.69356565e-20, 1.00000000e+00],
           [1.51870836e-07, 9.99999881e-01],
           [9.97794628e-01, 2.20544380e-03],
           [1.47506126e-13, 1.00000000e+00],
           [8.06490341e-09, 1.00000000e+00],
           [7.40945969e-31, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [3.37659595e-10, 1.00000000e+00],
           [3.54612308e-13, 1.00000000e+00],
           [4.82334758e-28, 1.00000000e+00],
           [4.35218805e-18, 1.00000000e+00],
           [1.05291473e-17, 1.00000000e+00],
           [4.70380897e-33, 1.00000000e+00],
           [6.44774234e-04, 9.99355257e-01],
           [0.00000000e+00, 1.00000000e+00],
           [1.61130135e-17, 1.00000000e+00],
           [2.36595774e-08, 1.00000000e+00],
           [5.79376602e-30, 1.00000000e+00],
           [4.83359886e-24, 1.00000000e+00],
           [1.00000000e+00, 2.51006060e-08],
           [1.85502251e-23, 1.00000000e+00],
           [4.40029018e-02, 9.55997109e-01],
           [5.80397397e-34, 1.00000000e+00],
           [2.36382398e-12, 1.00000000e+00],
           [1.85534859e-07, 9.99999762e-01],
           [4.11947820e-21, 1.00000000e+00],
           [2.78259889e-35, 1.00000000e+00],
           [1.59123524e-25, 1.00000000e+00],
           [8.97579866e-06, 9.99991059e-01],
           [5.98645343e-19, 1.00000000e+00],
           [1.63567524e-08, 1.00000000e+00],
           [8.63184221e-03, 9.91368115e-01],
           [1.24529254e-06, 9.99998808e-01],
           [1.91458821e-06, 9.99998093e-01],
           [4.54754101e-09, 1.00000000e+00],
           [1.31700185e-12, 1.00000000e+00],
           [2.74692114e-34, 1.00000000e+00],
           [2.66849920e-02, 9.73315001e-01],
           [5.43736775e-14, 1.00000000e+00],
           [2.69135516e-35, 1.00000000e+00],
           [9.82973456e-01, 1.70264877e-02],
           [7.48563111e-01, 2.51436919e-01],
           [2.35748576e-06, 9.99997616e-01],
           [2.92037046e-19, 1.00000000e+00],
           [1.07818846e-24, 1.00000000e+00],
           [9.45719421e-01, 5.42806014e-02],
           [8.14972168e-16, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [2.81323998e-36, 1.00000000e+00],
           [6.69130001e-34, 1.00000000e+00],
           [4.89899170e-26, 1.00000000e+00],
           [1.59269026e-28, 1.00000000e+00],
           [1.20222708e-02, 9.87977743e-01],
           [1.24753514e-35, 1.00000000e+00],
           [2.30458107e-32, 1.00000000e+00],
           [4.44273055e-02, 9.55572724e-01],
           [2.27663685e-02, 9.77233648e-01],
           [3.25060013e-04, 9.99674916e-01],
           [5.80407464e-30, 1.00000000e+00],
           [3.09830284e-07, 9.99999642e-01],
           [2.42872736e-11, 1.00000000e+00],
           [9.29018905e-21, 1.00000000e+00],
           [1.60901518e-11, 1.00000000e+00],
           [9.66310130e-22, 1.00000000e+00],
           [4.07135907e-14, 1.00000000e+00],
           [1.41696975e-01, 8.58303070e-01],
           [3.63842719e-06, 9.99996305e-01],
           [3.43545471e-05, 9.99965668e-01],
           [2.04467485e-11, 1.00000000e+00],
           [6.93670074e-07, 9.99999285e-01],
           [3.27539501e-17, 1.00000000e+00],
           [8.59263268e-12, 1.00000000e+00],
           [3.04179908e-16, 1.00000000e+00],
           [7.49504023e-20, 1.00000000e+00],
           [2.76196829e-07, 9.99999762e-01],
           [6.12524737e-24, 1.00000000e+00],
           [3.45628630e-18, 1.00000000e+00],
           [6.78347660e-30, 1.00000000e+00],
           [1.51805304e-12, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00603215e-11, 1.00000000e+00],
           [3.29218954e-01, 6.70781076e-01],
           [1.69102979e-14, 1.00000000e+00],
           [9.41127382e-07, 9.99999046e-01],
           [9.63899493e-01, 3.61005142e-02],
           [3.27862608e-21, 1.00000000e+00],
           [3.89920524e-06, 9.99996066e-01],
           [0.00000000e+00, 1.00000000e+00],
           [3.83166084e-03, 9.96168315e-01],
           [3.60312353e-19, 1.00000000e+00],
           [2.59789452e-02, 9.74021018e-01],
           [7.27146435e-18, 1.00000000e+00],
           [5.00670694e-06, 9.99994993e-01],
           [1.08623143e-22, 1.00000000e+00],
           [3.42918402e-12, 1.00000000e+00],
           [9.99941707e-01, 5.82355351e-05],
           [9.99101162e-01, 8.98887054e-04],
           [2.70419673e-14, 1.00000000e+00],
           [3.35294269e-02, 9.66470599e-01],
           [0.00000000e+00, 1.00000000e+00],
           [1.01861551e-01, 8.98138463e-01],
           [3.70508395e-02, 9.62949097e-01],
           [3.22347030e-25, 1.00000000e+00],
           [9.98205900e-01, 1.79413706e-03],
           [3.12307691e-07, 9.99999642e-01],
           [2.41287047e-12, 1.00000000e+00],
           [1.86755676e-02, 9.81324494e-01],
           [4.00092657e-19, 1.00000000e+00],
           [2.39910393e-15, 1.00000000e+00],
           [4.71401682e-31, 1.00000000e+00],
           [3.45401669e-27, 1.00000000e+00],
           [2.05203691e-17, 1.00000000e+00],
           [5.49402068e-35, 1.00000000e+00],
           [5.01396653e-17, 1.00000000e+00],
           [6.30859234e-35, 1.00000000e+00],
           [7.56974369e-02, 9.24302578e-01],
           [4.78105271e-08, 1.00000000e+00],
           [4.55227689e-10, 1.00000000e+00],
           [3.10513321e-30, 1.00000000e+00],
           [2.17386249e-13, 1.00000000e+00],
           [1.70913950e-07, 9.99999881e-01],
           [1.81329062e-06, 9.99998212e-01],
           [3.20291883e-11, 1.00000000e+00],
           [2.20910401e-08, 1.00000000e+00],
           [9.97673452e-01, 2.32652132e-03],
           [3.40278816e-28, 1.00000000e+00],
           [4.04551316e-25, 1.00000000e+00],
           [4.42839344e-23, 1.00000000e+00],
           [4.74402764e-19, 1.00000000e+00],
           [3.27902347e-01, 6.72097623e-01],
           [2.89310576e-18, 1.00000000e+00],
           [1.33961846e-08, 1.00000000e+00],
           [9.13092518e-13, 1.00000000e+00],
           [1.69039599e-16, 1.00000000e+00],
           [9.96517777e-01, 3.48219112e-03],
           [4.32112320e-05, 9.99956846e-01],
           [1.44702458e-10, 1.00000000e+00],
           [9.20984711e-09, 1.00000000e+00],
           [1.61002465e-08, 1.00000000e+00],
           [3.81042335e-33, 1.00000000e+00],
           [8.12032144e-19, 1.00000000e+00],
           [5.21709859e-01, 4.78290141e-01],
           [6.50857457e-09, 1.00000000e+00],
           [1.12325967e-13, 1.00000000e+00],
           [1.72629852e-28, 1.00000000e+00],
           [8.60352429e-06, 9.99991417e-01],
           [0.00000000e+00, 1.00000000e+00],
           [6.86301414e-24, 1.00000000e+00],
           [1.02827798e-07, 9.99999881e-01],
           [1.63621785e-12, 1.00000000e+00],
           [9.98226941e-01, 1.77307497e-03],
           [1.33809717e-22, 1.00000000e+00],
           [6.04957044e-02, 9.39504266e-01],
           [3.33123109e-31, 1.00000000e+00],
           [5.49287504e-10, 1.00000000e+00],
           [1.05080762e-05, 9.99989510e-01],
           [0.00000000e+00, 1.00000000e+00],
           [1.82820719e-16, 1.00000000e+00],
           [1.51739368e-11, 1.00000000e+00],
           [1.11963668e-27, 1.00000000e+00],
           [2.43164686e-05, 9.99975681e-01],
           [1.06737383e-14, 1.00000000e+00],
           [8.85298610e-01, 1.14701383e-01],
           [4.12017018e-01, 5.87982953e-01],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [3.52860610e-14, 1.00000000e+00],
           [2.54158357e-18, 1.00000000e+00],
           [3.02055560e-06, 9.99997020e-01],
           [0.00000000e+00, 1.00000000e+00],
           [2.47266903e-19, 1.00000000e+00],
           [9.86803949e-01, 1.31960260e-02],
           [1.41308680e-02, 9.85869169e-01],
           [2.22024395e-11, 1.00000000e+00],
           [1.10415785e-15, 1.00000000e+00],
           [3.12303690e-14, 1.00000000e+00],
           [7.58978951e-19, 1.00000000e+00],
           [6.06760996e-12, 1.00000000e+00],
           [2.87540956e-06, 9.99997139e-01],
           [4.67736925e-07, 9.99999523e-01],
           [2.92787390e-05, 9.99970675e-01],
           [5.00014641e-10, 1.00000000e+00],
           [3.29232368e-29, 1.00000000e+00],
           [1.15785548e-35, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [3.45877034e-08, 1.00000000e+00],
           [5.86867452e-01, 4.13132608e-01],
           [9.99970436e-01, 2.95208374e-05],
           [0.00000000e+00, 1.00000000e+00],
           [1.10820230e-19, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [4.65235731e-04, 9.99534845e-01],
           [2.36163978e-22, 1.00000000e+00],
           [1.80354729e-16, 1.00000000e+00],
           [2.10710077e-05, 9.99978900e-01],
           [1.85281933e-27, 1.00000000e+00],
           [3.15425632e-06, 9.99996901e-01],
           [1.03982334e-08, 1.00000000e+00],
           [2.76830851e-05, 9.99972343e-01],
           [7.04793446e-10, 1.00000000e+00],
           [3.91335727e-14, 1.00000000e+00],
           [5.45841575e-23, 1.00000000e+00],
           [9.92957592e-01, 7.04243314e-03],
           [3.11352802e-03, 9.96886432e-01],
           [1.72023485e-16, 1.00000000e+00],
           [3.61168085e-35, 1.00000000e+00],
           [1.27826277e-31, 1.00000000e+00],
           [9.08079278e-03, 9.90919232e-01],
           [6.82489372e-06, 9.99993205e-01],
           [2.99504453e-13, 1.00000000e+00],
           [2.78319773e-17, 1.00000000e+00],
           [4.07521799e-19, 1.00000000e+00],
           [3.34594956e-11, 1.00000000e+00],
           [2.88101999e-21, 1.00000000e+00],
           [3.77278706e-19, 1.00000000e+00],
           [1.00559247e-23, 1.00000000e+00],
           [2.11224324e-05, 9.99978900e-01],
           [6.03785837e-16, 1.00000000e+00],
           [4.04402629e-28, 1.00000000e+00],
           [8.38175572e-21, 1.00000000e+00],
           [3.05541069e-03, 9.96944606e-01],
           [3.22557189e-11, 1.00000000e+00],
           [1.89772429e-04, 9.99810278e-01],
           [1.97240912e-09, 1.00000000e+00],
           [6.77161704e-09, 1.00000000e+00],
           [7.98566333e-08, 9.99999881e-01],
           [9.99904275e-01, 9.56800504e-05],
           [9.07407939e-01, 9.25920606e-02],
           [1.11396690e-11, 1.00000000e+00],
           [1.36360869e-08, 1.00000000e+00],
           [1.00283569e-03, 9.98997152e-01],
           [1.19465249e-05, 9.99988079e-01],
           [3.29010344e-28, 1.00000000e+00],
           [9.99435723e-01, 5.64238406e-04],
           [5.94060662e-14, 1.00000000e+00],
           [5.35931440e-06, 9.99994636e-01],
           [0.00000000e+00, 1.00000000e+00],
           [6.77450134e-06, 9.99993205e-01],
           [0.00000000e+00, 1.00000000e+00],
           [2.76722959e-12, 1.00000000e+00],
           [1.02189586e-23, 1.00000000e+00],
           [2.39746038e-20, 1.00000000e+00],
           [2.74492133e-26, 1.00000000e+00],
           [1.06818931e-35, 1.00000000e+00],
           [3.99794427e-15, 1.00000000e+00],
           [8.69230794e-13, 1.00000000e+00],
           [1.02184909e-08, 1.00000000e+00],
           [9.99998212e-01, 1.84608746e-06],
           [3.32118336e-15, 1.00000000e+00],
           [5.95519427e-37, 1.00000000e+00],
           [1.14923060e-09, 1.00000000e+00],
           [4.28136140e-08, 1.00000000e+00],
           [1.32280981e-12, 1.00000000e+00],
           [9.99999404e-01, 5.56592397e-07],
           [8.95787299e-01, 1.04212701e-01],
           [2.50317733e-22, 1.00000000e+00],
           [1.58528759e-15, 1.00000000e+00],
           [1.71806849e-26, 1.00000000e+00],
           [3.39917286e-04, 9.99660134e-01],
           [6.20285682e-02, 9.37971473e-01],
           [1.07559794e-21, 1.00000000e+00],
           [6.55405894e-02, 9.34459329e-01],
           [5.84935246e-04, 9.99415040e-01],
           [1.21379772e-19, 1.00000000e+00],
           [2.67626089e-03, 9.97323751e-01],
           [1.56086318e-17, 1.00000000e+00],
           [1.50802434e-05, 9.99984860e-01],
           [3.70871004e-15, 1.00000000e+00],
           [9.98270631e-01, 1.72939221e-03],
           [1.94033127e-08, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [0.00000000e+00, 1.00000000e+00],
           [1.00045349e-31, 1.00000000e+00],
           [3.10924460e-16, 1.00000000e+00],
           [8.87042495e-10, 1.00000000e+00],
           [9.68069740e-30, 1.00000000e+00],
           [3.72636976e-04, 9.99627352e-01],
           [1.38424894e-14, 1.00000000e+00],
           [3.64441866e-24, 1.00000000e+00]], dtype=float32)

    lst=[]

    for i in range(0,len(y_pred)):
         k=np.argmax(y_pred[i]) #it gives index value of the highest probability for each iteration 
         print(k)
         lst.append(k)

    y_pred_label=np.array(lst)

    1
    1
    1
    0
    0
    1
    1
    1
    1
    1
    1
    1
    0
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    0
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    0
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    0
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    0
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    0
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1

    from sklearn.metrics import confusion_matrix
    from sklearn.metrics import plot_confusion_matrix

    mat = confusion_matrix(Y_test, y_pred_label) #we dont do this because we dont get the whole number on the confusion matrixis to fet the whole number annotation
    sns.heatmap(mat, square=True, annot=True, cbar=False)
    plt.xlabel('predicted value')
    plt.ylabel('true value');

[]

    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


    print('Accuracy: %.3f' % accuracy_score(y_true=Y_test, y_pred=y_pred_label))
    print('Precision: %.3f' % precision_score(y_true=Y_test, y_pred=y_pred_label))
    print('Recall: %.3f' % recall_score(y_true=Y_test, y_pred=y_pred_label))
    print('F1: %.3f' % f1_score(y_true=Y_test, y_pred=y_pred_label))

    Accuracy: 0.590
    Precision: 0.634
    Recall: 0.885
    F1: 0.739

    from sklearn import metrics

    # Model f1_score: how often is the classifier correct?
    DenseNet121_f1_score=metrics.f1_score(Y_test, y_pred_label)



    print("F1_score:",DenseNet121_f1_score)

    F1_score: 0.7386666666666666

    from sklearn.metrics import roc_curve
    from sklearn.metrics import auc
    fpr_keras, tpr_keras, thresholds_keras = roc_curve(Y_test, y_pred_label)


    auc_keras_DenseNet121 = auc(fpr_keras, tpr_keras)
    auc_keras_DenseNet121 #auc score

    0.45764352793106783

    import matplotlib.pyplot as plt
    plt.title('DenseNet121-Receiver Operating Characteristic')
    plt.plot(fpr_keras, tpr_keras, color='green',marker='o', label = 'AUC area = %0.2f' % auc_keras_DenseNet121)
    plt.legend(loc = 'lower right')
    plt.plot([0, 1], [0, 1],'r--') #diagonal line
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')

    Text(0.5, 0, 'False Positive Rate')

[]

Model-EfficientNetB2

    from tensorflow.keras.applications.efficientnet import EfficientNetB2

    input_shape=(224,224,3)



    head_model = EfficientNetB2(include_top=False,
                     weights='imagenet',
                     #input_tensor=None,
                     input_shape=input_shape)
                     #pooling='avg',
                     #classes=2,
                     #classifier_activation='softmax')



    for layer in head_model.layers:
      layer.trainable = False #trainable are the last three layers until flatten (the whole set of fully connected layers)


    x = layers.Flatten()(head_model.output) #google: how to cut off a pre train model resnet and add fully connected layers in tensorflow
    x = layers.Dense(1000, activation='relu')(x)
    predictions = layers.Dense(2, activation = 'softmax')(x)

    model = Model(inputs = head_model.input, outputs = predictions)






    model.compile(optimizer=tf.keras.optimizers.Adam(0.00001),
                  loss=SparseCategoricalCrossentropy(from_logits=True),
                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])

    history=model.fit(
        X_train,Y_train,
        epochs=25, #can change the epoch
        validation_split=0.15, verbose=1,callbacks=[es])

    Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb2_notop.h5
    31793152/31790344 [==============================] - 1s 0us/step
    31801344/31790344 [==============================] - 1s 0us/step
    Epoch 1/25

    /Users/camron/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: "`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?"
      return dispatch_target(*args, **kwargs)

    51/51 [==============================] - 114s 2s/step - loss: 0.8784 - sparse_categorical_accuracy: 0.5089 - val_loss: 0.6510 - val_sparse_categorical_accuracy: 0.6725
    Epoch 2/25
    51/51 [==============================] - 106s 2s/step - loss: 0.7220 - sparse_categorical_accuracy: 0.5503 - val_loss: 0.6489 - val_sparse_categorical_accuracy: 0.6725
    Epoch 3/25
    51/51 [==============================] - 106s 2s/step - loss: 0.7183 - sparse_categorical_accuracy: 0.5577 - val_loss: 0.6324 - val_sparse_categorical_accuracy: 0.6725
    Epoch 4/25
    51/51 [==============================] - 106s 2s/step - loss: 0.6974 - sparse_categorical_accuracy: 0.5638 - val_loss: 0.6584 - val_sparse_categorical_accuracy: 0.6725
    Epoch 5/25
    51/51 [==============================] - 106s 2s/step - loss: 0.7108 - sparse_categorical_accuracy: 0.5651 - val_loss: 0.7380 - val_sparse_categorical_accuracy: 0.3275
    Epoch 6/25
    51/51 [==============================] - 104s 2s/step - loss: 0.8088 - sparse_categorical_accuracy: 0.5163 - val_loss: 0.6338 - val_sparse_categorical_accuracy: 0.6725
    Epoch 7/25
    51/51 [==============================] - 104s 2s/step - loss: 0.7077 - sparse_categorical_accuracy: 0.5558 - val_loss: 0.7163 - val_sparse_categorical_accuracy: 0.3275
    Epoch 8/25
    51/51 [==============================] - 104s 2s/step - loss: 0.7460 - sparse_categorical_accuracy: 0.5620 - val_loss: 0.8227 - val_sparse_categorical_accuracy: 0.6725
    Epoch 9/25
    51/51 [==============================] - 103s 2s/step - loss: 0.7206 - sparse_categorical_accuracy: 0.5447 - val_loss: 0.6370 - val_sparse_categorical_accuracy: 0.6725
    Epoch 10/25
    51/51 [==============================] - 104s 2s/step - loss: 0.8122 - sparse_categorical_accuracy: 0.5392 - val_loss: 0.8885 - val_sparse_categorical_accuracy: 0.3275
    Epoch 11/25
    51/51 [==============================] - 104s 2s/step - loss: 0.7654 - sparse_categorical_accuracy: 0.5410 - val_loss: 0.8179 - val_sparse_categorical_accuracy: 0.6725
    Epoch 12/25
    51/51 [==============================] - 104s 2s/step - loss: 0.7761 - sparse_categorical_accuracy: 0.5429 - val_loss: 0.9303 - val_sparse_categorical_accuracy: 0.3275
    Epoch 13/25
    51/51 [==============================] - 104s 2s/step - loss: 0.7018 - sparse_categorical_accuracy: 0.5583 - val_loss: 0.6368 - val_sparse_categorical_accuracy: 0.6725
    Epoch 14/25
    51/51 [==============================] - 105s 2s/step - loss: 0.7557 - sparse_categorical_accuracy: 0.5441 - val_loss: 0.7064 - val_sparse_categorical_accuracy: 0.3275
    Epoch 15/25
    51/51 [==============================] - 106s 2s/step - loss: 0.7113 - sparse_categorical_accuracy: 0.5651 - val_loss: 0.6324 - val_sparse_categorical_accuracy: 0.6725
    Epoch 16/25
    51/51 [==============================] - 106s 2s/step - loss: 0.7055 - sparse_categorical_accuracy: 0.5725 - val_loss: 0.6337 - val_sparse_categorical_accuracy: 0.6725
    Epoch 17/25
    51/51 [==============================] - 106s 2s/step - loss: 0.7216 - sparse_categorical_accuracy: 0.5441 - val_loss: 0.6761 - val_sparse_categorical_accuracy: 0.6725
    Epoch 18/25
    51/51 [==============================] - 108s 2s/step - loss: 0.7521 - sparse_categorical_accuracy: 0.5429 - val_loss: 0.6688 - val_sparse_categorical_accuracy: 0.6725
    Epoch 19/25
    51/51 [==============================] - 105s 2s/step - loss: 0.7918 - sparse_categorical_accuracy: 0.5176 - val_loss: 0.6504 - val_sparse_categorical_accuracy: 0.6725
    Epoch 20/25
    51/51 [==============================] - 105s 2s/step - loss: 0.7089 - sparse_categorical_accuracy: 0.5651 - val_loss: 0.7607 - val_sparse_categorical_accuracy: 0.3275
    Epoch 21/25
    51/51 [==============================] - 106s 2s/step - loss: 0.7795 - sparse_categorical_accuracy: 0.5589 - val_loss: 0.7486 - val_sparse_categorical_accuracy: 0.6725
    Epoch 22/25
    51/51 [==============================] - 107s 2s/step - loss: 0.7227 - sparse_categorical_accuracy: 0.5515 - val_loss: 0.8386 - val_sparse_categorical_accuracy: 0.3275
    Epoch 23/25
    51/51 [==============================] - 104s 2s/step - loss: 0.7049 - sparse_categorical_accuracy: 0.5484 - val_loss: 0.6728 - val_sparse_categorical_accuracy: 0.6725
    Epoch 24/25
    51/51 [==============================] - 102s 2s/step - loss: 0.6950 - sparse_categorical_accuracy: 0.5706 - val_loss: 0.6502 - val_sparse_categorical_accuracy: 0.6725
    Epoch 25/25
    51/51 [==============================] - 101s 2s/step - loss: 0.7120 - sparse_categorical_accuracy: 0.5577 - val_loss: 0.6430 - val_sparse_categorical_accuracy: 0.6725

    plt.plot(history.history['sparse_categorical_accuracy'])
    plt.plot(history.history['val_sparse_categorical_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()

[]

    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()

[]

    y_pred=model.predict(X_test) 
    y_pred

    array([[0.59464335, 0.40535668],
           [0.7628341 , 0.23716593],
           [0.7519526 , 0.24804741],
           [0.30144808, 0.6985519 ],
           [0.8625482 , 0.13745181],
           [0.9829731 , 0.01702692],
           [0.51615167, 0.4838484 ],
           [0.47131103, 0.52868897],
           [0.3847355 , 0.61526453],
           [0.8462638 , 0.15373613],
           [0.3216226 , 0.67837733],
           [0.63825655, 0.3617435 ],
           [0.86790246, 0.13209753],
           [0.2972145 , 0.7027855 ],
           [0.62728745, 0.37271264],
           [0.348882  , 0.65111804],
           [0.80626106, 0.19373891],
           [0.26048857, 0.73951143],
           [0.479717  , 0.52028304],
           [0.6057601 , 0.3942399 ],
           [0.37073827, 0.62926173],
           [0.40153667, 0.59846336],
           [0.8170565 , 0.18294354],
           [0.669744  , 0.33025596],
           [0.71036345, 0.28963658],
           [0.42078897, 0.57921106],
           [0.51770914, 0.4822909 ],
           [0.10247595, 0.89752406],
           [0.25750965, 0.74249035],
           [0.56390166, 0.43609828],
           [0.29814312, 0.70185685],
           [0.9352221 , 0.06477792],
           [0.5592843 , 0.44071567],
           [0.22617315, 0.77382684],
           [0.38289914, 0.61710083],
           [0.20608543, 0.7939146 ],
           [0.5611018 , 0.43889824],
           [0.90212405, 0.09787595],
           [0.23736207, 0.7626379 ],
           [0.5369721 , 0.46302792],
           [0.81090915, 0.18909085],
           [0.5253382 , 0.47466183],
           [0.577299  , 0.42270103],
           [0.5813451 , 0.4186549 ],
           [0.22898756, 0.7710125 ],
           [0.94993985, 0.05006015],
           [0.37914   , 0.62086004],
           [0.44237638, 0.5576236 ],
           [0.3822579 , 0.6177421 ],
           [0.56278324, 0.4372168 ],
           [0.17168805, 0.8283119 ],
           [0.24466845, 0.7553316 ],
           [0.6084653 , 0.39153475],
           [0.58486325, 0.41513678],
           [0.73649055, 0.2635095 ],
           [0.60289335, 0.39710665],
           [0.4883123 , 0.51168764],
           [0.25814965, 0.7418504 ],
           [0.44703713, 0.55296284],
           [0.5650671 , 0.4349329 ],
           [0.7083289 , 0.2916711 ],
           [0.894623  , 0.10537701],
           [0.19208324, 0.80791676],
           [0.5794191 , 0.42058095],
           [0.89517194, 0.10482808],
           [0.62637377, 0.37362623],
           [0.28219116, 0.7178088 ],
           [0.5504682 , 0.4495318 ],
           [0.8874289 , 0.11257111],
           [0.14968492, 0.85031503],
           [0.2995422 , 0.7004578 ],
           [0.83568734, 0.16431266],
           [0.21125469, 0.78874534],
           [0.57832295, 0.421677  ],
           [0.45385197, 0.546148  ],
           [0.8773621 , 0.12263788],
           [0.78192675, 0.21807323],
           [0.97015005, 0.02984991],
           [0.47318673, 0.52681327],
           [0.26450774, 0.7354923 ],
           [0.5660739 , 0.43392617],
           [0.25203025, 0.74796975],
           [0.61374027, 0.3862597 ],
           [0.59998995, 0.40001008],
           [0.25046602, 0.749534  ],
           [0.4246561 , 0.5753439 ],
           [0.86851645, 0.13148348],
           [0.41524696, 0.58475304],
           [0.4000996 , 0.59990036],
           [0.7156618 , 0.28433815],
           [0.66580373, 0.33419627],
           [0.4428932 , 0.5571068 ],
           [0.6015503 , 0.39844975],
           [0.42542094, 0.57457906],
           [0.6756911 , 0.32430893],
           [0.5122442 , 0.48775578],
           [0.62581855, 0.3741815 ],
           [0.29483077, 0.70516926],
           [0.27219513, 0.7278049 ],
           [0.15844983, 0.8415502 ],
           [0.9124894 , 0.08751064],
           [0.95613956, 0.04386048],
           [0.278177  , 0.721823  ],
           [0.14513609, 0.85486394],
           [0.47148415, 0.5285158 ],
           [0.717137  , 0.28286305],
           [0.35817423, 0.6418257 ],
           [0.5673718 , 0.4326282 ],
           [0.75033265, 0.24966733],
           [0.72049016, 0.2795098 ],
           [0.75587547, 0.24412452],
           [0.562341  , 0.43765903],
           [0.3396041 , 0.6603959 ],
           [0.7630488 , 0.23695117],
           [0.603562  , 0.396438  ],
           [0.2254432 , 0.77455676],
           [0.5386834 , 0.46131653],
           [0.16886109, 0.83113885],
           [0.5022211 , 0.49777886],
           [0.91185534, 0.08814459],
           [0.7765813 , 0.22341867],
           [0.89315224, 0.10684776],
           [0.8059901 , 0.19400989],
           [0.32465756, 0.67534244],
           [0.4184188 , 0.5815812 ],
           [0.45876813, 0.5412319 ],
           [0.51747864, 0.4825214 ],
           [0.49324456, 0.5067554 ],
           [0.30042884, 0.69957113],
           [0.39737773, 0.6026223 ],
           [0.90034014, 0.09965983],
           [0.46966502, 0.530335  ],
           [0.8845081 , 0.11549196],
           [0.5931741 , 0.4068259 ],
           [0.594849  , 0.40515098],
           [0.6360273 , 0.36397275],
           [0.45431235, 0.5456876 ],
           [0.2534246 , 0.74657536],
           [0.78071284, 0.21928716],
           [0.71914726, 0.28085282],
           [0.62157214, 0.37842783],
           [0.71717334, 0.2828267 ],
           [0.68179154, 0.31820837],
           [0.37515035, 0.6248497 ],
           [0.23255146, 0.76744854],
           [0.42134324, 0.57865673],
           [0.8957752 , 0.10422476],
           [0.39935184, 0.6006481 ],
           [0.5401692 , 0.45983082],
           [0.6863327 , 0.3136673 ],
           [0.69295496, 0.30704498],
           [0.44477916, 0.55522084],
           [0.6109047 , 0.38909528],
           [0.281696  , 0.71830404],
           [0.771152  , 0.22884794],
           [0.26700425, 0.73299575],
           [0.31445175, 0.6855483 ],
           [0.7731945 , 0.22680552],
           [0.30586475, 0.6941352 ],
           [0.7864706 , 0.21352936],
           [0.47364876, 0.5263512 ],
           [0.66225   , 0.33775002],
           [0.263526  , 0.736474  ],
           [0.56905013, 0.43094984],
           [0.2696388 , 0.73036116],
           [0.6311078 , 0.3688922 ],
           [0.42533907, 0.57466096],
           [0.46600106, 0.5339989 ],
           [0.7560886 , 0.24391137],
           [0.34311497, 0.656885  ],
           [0.28516898, 0.71483105],
           [0.2540921 , 0.7459079 ],
           [0.51649374, 0.48350623],
           [0.59857315, 0.40142676],
           [0.25995204, 0.740048  ],
           [0.79850405, 0.20149586],
           [0.359872  , 0.64012796],
           [0.5303198 , 0.46968016],
           [0.7114273 , 0.28857282],
           [0.51068395, 0.48931602],
           [0.8636165 , 0.13638343],
           [0.2910148 , 0.7089852 ],
           [0.6137708 , 0.3862292 ],
           [0.7286701 , 0.27132985],
           [0.33148402, 0.668516  ],
           [0.7538336 , 0.24616641],
           [0.923779  , 0.07622102],
           [0.47208333, 0.52791667],
           [0.6596131 , 0.3403869 ],
           [0.23503548, 0.7649645 ],
           [0.5630597 , 0.4369403 ],
           [0.74508125, 0.25491875],
           [0.6787183 , 0.3212817 ],
           [0.7140261 , 0.2859739 ],
           [0.5066    , 0.49340004],
           [0.5526557 , 0.44734427],
           [0.351642  , 0.64835805],
           [0.84767705, 0.15232295],
           [0.58769757, 0.41230237],
           [0.78334904, 0.21665098],
           [0.7288688 , 0.27113116],
           [0.9390756 , 0.06092448],
           [0.60361814, 0.39638186],
           [0.48799238, 0.51200753],
           [0.6426616 , 0.35733846],
           [0.9355177 , 0.06448224],
           [0.35852814, 0.64147186],
           [0.75752085, 0.24247913],
           [0.60154444, 0.39845562],
           [0.80966663, 0.19033335],
           [0.6414484 , 0.35855162],
           [0.21043259, 0.7895674 ],
           [0.3580605 , 0.64193946],
           [0.5813928 , 0.41860718],
           [0.51535475, 0.48464516],
           [0.7239505 , 0.27604946],
           [0.6273726 , 0.37262735],
           [0.37443116, 0.62556887],
           [0.28908443, 0.71091557],
           [0.26558906, 0.73441094],
           [0.5184384 , 0.48156157],
           [0.6860017 , 0.3139983 ],
           [0.74446833, 0.25553167],
           [0.28224543, 0.7177545 ],
           [0.22523174, 0.77476823],
           [0.4853127 , 0.5146873 ],
           [0.71297574, 0.28702426],
           [0.90661454, 0.0933855 ],
           [0.67590976, 0.3240902 ],
           [0.8707358 , 0.1292642 ],
           [0.79159385, 0.20840614],
           [0.7020013 , 0.29799867],
           [0.5395513 , 0.46044874],
           [0.39566606, 0.60433394],
           [0.90619665, 0.09380331],
           [0.1304081 , 0.8695919 ],
           [0.8251867 , 0.17481327],
           [0.5925512 , 0.4074489 ],
           [0.9334627 , 0.06653736],
           [0.7983586 , 0.20164135],
           [0.6577441 , 0.34225592],
           [0.6472208 , 0.35277918],
           [0.40187213, 0.59812784],
           [0.57052135, 0.4294787 ],
           [0.19339827, 0.8066017 ],
           [0.42930186, 0.5706982 ],
           [0.66080034, 0.33919963],
           [0.5407708 , 0.45922914],
           [0.5931305 , 0.40686944],
           [0.27020904, 0.7297909 ],
           [0.5362024 , 0.4637976 ],
           [0.42300236, 0.5769976 ],
           [0.70115125, 0.29884875],
           [0.60358196, 0.39641804],
           [0.7084327 , 0.29156736],
           [0.51201826, 0.48798174],
           [0.8683259 , 0.1316741 ],
           [0.9508749 , 0.04912511],
           [0.7803127 , 0.21968734],
           [0.68249005, 0.31750992],
           [0.8829531 , 0.11704694],
           [0.898182  , 0.1018181 ],
           [0.7750791 , 0.2249209 ],
           [0.90435326, 0.09564677],
           [0.72120464, 0.2787954 ],
           [0.8326679 , 0.16733208],
           [0.58716035, 0.41283965],
           [0.21105868, 0.7889413 ],
           [0.83270067, 0.16729933],
           [0.4983402 , 0.5016598 ],
           [0.3111978 , 0.6888022 ],
           [0.303615  , 0.69638497],
           [0.50510395, 0.49489605],
           [0.41485873, 0.58514124],
           [0.37383518, 0.62616485],
           [0.759791  , 0.24020895],
           [0.27854374, 0.7214563 ],
           [0.31195626, 0.6880438 ],
           [0.96974033, 0.03025959],
           [0.7387598 , 0.26124018],
           [0.75727147, 0.24272849],
           [0.78809315, 0.21190679],
           [0.62164444, 0.37835556],
           [0.23531944, 0.7646805 ],
           [0.93297035, 0.06702965],
           [0.67439276, 0.32560718],
           [0.3715964 , 0.62840366],
           [0.45590207, 0.544098  ],
           [0.42410362, 0.5758964 ],
           [0.7161058 , 0.28389415],
           [0.7727308 , 0.2272692 ],
           [0.46877512, 0.53122485],
           [0.42593917, 0.5740608 ],
           [0.67385787, 0.32614213],
           [0.43679282, 0.56320715],
           [0.5261855 , 0.4738145 ],
           [0.5097948 , 0.49020523],
           [0.28470168, 0.7152983 ],
           [0.5425282 , 0.45747173],
           [0.77052224, 0.22947769],
           [0.3106331 , 0.6893669 ],
           [0.47344628, 0.5265537 ],
           [0.46517992, 0.53482014],
           [0.7806718 , 0.21932818],
           [0.36614278, 0.6338572 ],
           [0.49839717, 0.5016028 ],
           [0.3437146 , 0.6562854 ],
           [0.31506723, 0.68493277],
           [0.7095338 , 0.29046625],
           [0.6725215 , 0.3274785 ],
           [0.9026403 , 0.09735972],
           [0.3370048 , 0.6629952 ],
           [0.60938936, 0.3906106 ],
           [0.8738266 , 0.12617339],
           [0.8807576 , 0.11924247],
           [0.26762623, 0.7323737 ],
           [0.8411215 , 0.15887854],
           [0.21978368, 0.78021634],
           [0.48081428, 0.5191857 ],
           [0.39352757, 0.60647243],
           [0.58866876, 0.41133124],
           [0.82775563, 0.17224441],
           [0.4526813 , 0.54731876],
           [0.48878062, 0.5112193 ],
           [0.3605139 , 0.63948613],
           [0.4006023 , 0.5993977 ],
           [0.42602393, 0.5739761 ],
           [0.22021922, 0.7797808 ],
           [0.73146874, 0.2685312 ],
           [0.6348893 , 0.3651107 ],
           [0.48500556, 0.51499444],
           [0.51348364, 0.4865163 ],
           [0.24602741, 0.7539726 ],
           [0.82804376, 0.17195629],
           [0.87556833, 0.12443166],
           [0.76233095, 0.23766908],
           [0.8286144 , 0.17138556],
           [0.38878664, 0.6112133 ],
           [0.6383908 , 0.3616092 ],
           [0.3342449 , 0.66575515],
           [0.34188882, 0.6581112 ],
           [0.42067477, 0.57932526],
           [0.63555205, 0.36444792],
           [0.29242414, 0.7075759 ],
           [0.3225674 , 0.6774326 ],
           [0.63183874, 0.36816132],
           [0.68147403, 0.31852594],
           [0.84351575, 0.15648426],
           [0.45525217, 0.5447478 ],
           [0.72709715, 0.27290282],
           [0.56607616, 0.43392378],
           [0.3226363 , 0.6773637 ],
           [0.6226863 , 0.37731367],
           [0.32354546, 0.67645454],
           [0.86086637, 0.13913363],
           [0.28325152, 0.7167485 ],
           [0.5501526 , 0.4498474 ],
           [0.74770504, 0.252295  ],
           [0.3419989 , 0.65800107],
           [0.69085234, 0.30914772],
           [0.51577896, 0.48422104],
           [0.40789363, 0.5921064 ],
           [0.2566512 , 0.7433488 ],
           [0.9226041 , 0.07739592],
           [0.7916879 , 0.20831212],
           [0.689264  , 0.310736  ],
           [0.41967666, 0.5803233 ],
           [0.46403873, 0.53596133],
           [0.3023577 , 0.6976423 ],
           [0.8077826 , 0.19221741],
           [0.5547222 , 0.44527775],
           [0.32618508, 0.67381495],
           [0.5644349 , 0.43556502],
           [0.60405463, 0.39594543],
           [0.54767483, 0.45232514],
           [0.30767307, 0.69232696],
           [0.74862903, 0.25137094],
           [0.6706125 , 0.32938752],
           [0.375545  , 0.62445503],
           [0.2677267 , 0.7322733 ],
           [0.23765616, 0.7623439 ],
           [0.7305032 , 0.2694968 ],
           [0.48010635, 0.5198936 ],
           [0.83446234, 0.16553766],
           [0.2750403 , 0.7249597 ],
           [0.4812501 , 0.5187499 ],
           [0.66539174, 0.3346083 ],
           [0.6381265 , 0.3618735 ],
           [0.06889804, 0.931102  ],
           [0.5355776 , 0.4644224 ],
           [0.6934756 , 0.3065244 ],
           [0.52415377, 0.47584623],
           [0.22212708, 0.7778729 ],
           [0.41516915, 0.5848309 ],
           [0.8034267 , 0.19657329],
           [0.36708304, 0.632917  ],
           [0.76315916, 0.23684087],
           [0.31648517, 0.68351483],
           [0.22327714, 0.77672285],
           [0.34330937, 0.6566906 ],
           [0.56192935, 0.43807063],
           [0.5649136 , 0.43508652],
           [0.5828817 , 0.41711828],
           [0.41185904, 0.588141  ],
           [0.2892604 , 0.7107396 ],
           [0.39110732, 0.6088927 ],
           [0.7977151 , 0.20228481],
           [0.20670371, 0.79329634],
           [0.30390394, 0.69609606],
           [0.7676361 , 0.23236383],
           [0.45284924, 0.5471508 ],
           [0.19537345, 0.8046266 ],
           [0.3405612 , 0.65943885],
           [0.53024817, 0.46975175],
           [0.93198377, 0.06801627],
           [0.63486844, 0.36513162],
           [0.21589243, 0.7841075 ],
           [0.23011373, 0.76988626],
           [0.81863433, 0.18136565],
           [0.5263502 , 0.4736498 ],
           [0.5882877 , 0.4117123 ],
           [0.74427783, 0.25572217],
           [0.71984583, 0.28015414],
           [0.36722767, 0.6327723 ],
           [0.7222354 , 0.27776465],
           [0.7633665 , 0.2366335 ],
           [0.42363393, 0.57636607],
           [0.8443431 , 0.15565684],
           [0.7864213 , 0.21357875],
           [0.49791077, 0.5020892 ],
           [0.3485202 , 0.65147984],
           [0.71850806, 0.28149194],
           [0.51903975, 0.48096028],
           [0.5866906 , 0.41330934],
           [0.17875238, 0.82124764],
           [0.7410122 , 0.2589878 ],
           [0.40162367, 0.59837633],
           [0.93207866, 0.06792141],
           [0.71610934, 0.28389063],
           [0.56392264, 0.43607727],
           [0.27526444, 0.72473556],
           [0.267515  , 0.732485  ],
           [0.6127634 , 0.38723657],
           [0.59029996, 0.40970007],
           [0.6982186 , 0.30178145],
           [0.74312556, 0.25687438],
           [0.4550155 , 0.5449845 ],
           [0.5395491 , 0.4604509 ],
           [0.8717672 , 0.1282328 ],
           [0.6475834 , 0.35241657],
           [0.7921257 , 0.20787434],
           [0.6095399 , 0.3904601 ],
           [0.5585104 , 0.44148958],
           [0.5632083 , 0.43679175],
           [0.2301921 , 0.76980793],
           [0.25646535, 0.7435346 ],
           [0.6404375 , 0.35956252],
           [0.48953563, 0.5104644 ],
           [0.40507096, 0.59492904],
           [0.7439053 , 0.25609475],
           [0.95892084, 0.04107916],
           [0.8474596 , 0.1525404 ],
           [0.5812647 , 0.41873533],
           [0.69373214, 0.30626783],
           [0.75099313, 0.24900693],
           [0.5435315 , 0.45646858],
           [0.23716275, 0.76283723],
           [0.6479712 , 0.3520288 ],
           [0.4423575 , 0.5576426 ],
           [0.3704129 , 0.62958705],
           [0.4202752 , 0.57972485],
           [0.8518165 , 0.14818352],
           [0.525284  , 0.474716  ],
           [0.63762814, 0.3623719 ],
           [0.63540536, 0.36459467],
           [0.28061324, 0.71938676],
           [0.5982838 , 0.40171614],
           [0.57788116, 0.4221188 ]], dtype=float32)

    lst=[]

    for i in range(0,len(y_pred)):
         k=np.argmax(y_pred[i]) #it gives index value of the highest probability for each iteration 
         print(k)
         lst.append(k)

    y_pred_label=np.array(lst)

    0
    0
    0
    1
    0
    0
    0
    1
    1
    0
    1
    0
    0
    1
    0
    1
    0
    1
    1
    0
    1
    1
    0
    0
    0
    1
    0
    1
    1
    0
    1
    0
    0
    1
    1
    1
    0
    0
    1
    0
    0
    0
    0
    0
    1
    0
    1
    1
    1
    0
    1
    1
    0
    0
    0
    0
    1
    1
    1
    0
    0
    0
    1
    0
    0
    0
    1
    0
    0
    1
    1
    0
    1
    0
    1
    0
    0
    0
    1
    1
    0
    1
    0
    0
    1
    1
    0
    1
    1
    0
    0
    1
    0
    1
    0
    0
    0
    1
    1
    1
    0
    0
    1
    1
    1
    0
    1
    0
    0
    0
    0
    0
    1
    0
    0
    1
    0
    1
    0
    0
    0
    0
    0
    1
    1
    1
    0
    1
    1
    1
    0
    1
    0
    0
    0
    0
    1
    1
    0
    0
    0
    0
    0
    1
    1
    1
    0
    1
    0
    0
    0
    1
    0
    1
    0
    1
    1
    0
    1
    0
    1
    0
    1
    0
    1
    0
    1
    1
    0
    1
    1
    1
    0
    0
    1
    0
    1
    0
    0
    0
    0
    1
    0
    0
    1
    0
    0
    1
    0
    1
    0
    0
    0
    0
    0
    0
    1
    0
    0
    0
    0
    0
    0
    1
    0
    0
    1
    0
    0
    0
    0
    1
    1
    0
    0
    0
    0
    1
    1
    1
    0
    0
    0
    1
    1
    1
    0
    0
    0
    0
    0
    0
    0
    1
    0
    1
    0
    0
    0
    0
    0
    0
    1
    0
    1
    1
    0
    0
    0
    1
    0
    1
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    0
    1
    0
    1
    1
    1
    0
    1
    1
    0
    1
    1
    0
    0
    0
    0
    0
    1
    0
    0
    1
    1
    1
    0
    0
    1
    1
    0
    1
    0
    0
    1
    0
    0
    1
    1
    1
    0
    1
    1
    1
    1
    0
    0
    0
    1
    0
    0
    0
    1
    0
    1
    1
    1
    0
    0
    1
    1
    1
    1
    1
    1
    0
    0
    1
    0
    1
    0
    0
    0
    0
    1
    0
    1
    1
    1
    0
    1
    1
    0
    0
    0
    1
    0
    0
    1
    0
    1
    0
    1
    0
    0
    1
    0
    0
    1
    1
    0
    0
    0
    1
    1
    1
    0
    0
    1
    0
    0
    0
    1
    0
    0
    1
    1
    1
    0
    1
    0
    1
    1
    0
    0
    1
    0
    0
    0
    1
    1
    0
    1
    0
    1
    1
    1
    0
    0
    0
    1
    1
    1
    0
    1
    1
    0
    1
    1
    1
    0
    0
    0
    1
    1
    0
    0
    0
    0
    0
    1
    0
    0
    1
    0
    0
    1
    1
    0
    0
    0
    1
    0
    1
    0
    0
    0
    1
    1
    0
    0
    0
    0
    1
    0
    0
    0
    0
    0
    0
    0
    1
    1
    0
    1
    1
    0
    0
    0
    0
    0
    0
    0
    1
    0
    1
    1
    1
    0
    0
    0
    0
    1
    0
    0

    from sklearn.metrics import confusion_matrix
    from sklearn.metrics import plot_confusion_matrix

    mat = confusion_matrix(Y_test, y_pred_label) #we dont do this because we dont get the whole number on the confusion matrixis to fet the whole number annotation
    sns.heatmap(mat, square=True, annot=True, cbar=False)
    plt.xlabel('predicted value')
    plt.ylabel('true value');

[]

    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


    print('Accuracy: %.3f' % accuracy_score(y_true=Y_test, y_pred=y_pred_label))
    print('Precision: %.3f' % precision_score(y_true=Y_test, y_pred=y_pred_label))
    print('Recall: %.3f' % recall_score(y_true=Y_test, y_pred=y_pred_label))
    print('F1: %.3f' % f1_score(y_true=Y_test, y_pred=y_pred_label))

    Accuracy: 0.356
    Precision: 0.513
    Recall: 0.323
    F1: 0.396

    from sklearn import metrics

    # Model f1_score: how often is the classifier correct?
    EfficientNetB2_f1_score=metrics.f1_score(Y_test, y_pred_label)



    print("F1_score:",EfficientNetB2_f1_score)

    F1_score: 0.396078431372549

    from sklearn.metrics import roc_curve
    from sklearn.metrics import auc
    fpr_keras, tpr_keras, thresholds_keras = roc_curve(Y_test, y_pred_label)


    auc_keras_EfficientNetB2 = auc(fpr_keras, tpr_keras)
    auc_keras_EfficientNetB2 #auc score

    0.37043276212605286

    import matplotlib.pyplot as plt
    plt.title(' EfficientNetB2-Receiver Operating Characteristic')
    plt.plot(fpr_keras, tpr_keras, color='green',marker='o', label = 'AUC area = %0.2f' % auc_keras_EfficientNetB2)
    plt.legend(loc = 'lower right')
    plt.plot([0, 1], [0, 1],'r--') #diagonal line
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')

    Text(0.5, 0, 'False Positive Rate')

[]

Comparison

    F1_score_list=[baseline_f1_score,Resnet_f1_score,InceptionV3_f1_score,VGG16_f1_score,DenseNet121_f1_score,EfficientNetB2_f1_score]
    F1_score_list.sort()
    print(F1_score_list)
    classifier_names_list=["Baseline_Model","DenseNet121","EfficientNetB2","ResNet50","InceptionV3","VGG16"]

    [0.396078431372549, 0.5911111111111111, 0.7386666666666666, 0.7893401015228427, 0.7914032869785081, 0.8446455505279035]

    plt.style.use("fivethirtyeight")
    plt.figure(figsize=(12, 12))
    sns.barplot(x=classifier_names_list, y=F1_score_list)
    plt.xlabel("CNN Models")
    plt.ylabel("F1_Score")
    plt.xticks(rotation=45)
    plt.title("Model Comparison - F1_Score Accuracy")
    plt.show()

[]
